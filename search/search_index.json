{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"home/","title":"Welcome to My Docs","text":"<p>Note</p> <p>link to legacy page</p>"},{"location":"home/#tests","title":"tests","text":"<p>hello </p>"},{"location":"home/#sections","title":"Sections","text":"<ul> <li>Leetcode</li> <li>notes</li> </ul>"},{"location":"class_notes/CSE120/","title":"CSE 120","text":"<p>CSE 120. Operating Systems Principles (4) Tag: Systems</p> <p>Introduces operating systems concepts, including processes, synchronization, scheduling, memory management, virtual memory, file systems, and protection. May be coscheduled with CSE 220. Prerequisites: CSE 15L or CSE 29 and CSE 30 and CSE 100 and CSE 101; restricted to students within the CS25, CS26, CS27, CS29, and EC26 majors. All other students will be allowed as space permits.</p>"},{"location":"class_notes/CSE120/OS/","title":"Operating System","text":"<p>Kernal \\(\\approx\\) OS</p>"},{"location":"class_notes/CSE120/OS/#software-of-a-unix-system","title":"Software of a Unix System","text":""},{"location":"class_notes/CSE120/OS/#application","title":"Application","text":"<ul> <li>user code</li> <li>use library calls</li> </ul>"},{"location":"class_notes/CSE120/OS/#libraries","title":"Libraries","text":"<ul> <li>pre-compiled</li> <li>written by experts</li> </ul>"},{"location":"class_notes/CSE120/OS/#portable-os-layer","title":"Portable OS Layer","text":"<ul> <li>all high level codes</li> <li>system calls</li> </ul>"},{"location":"class_notes/CSE120/OS/#machine-dependent-layer","title":"machine-dependent layer","text":"<ul> <li>bootstrap</li> <li>IO device driver</li> <li>interrupts and exceptions</li> <li>mem mangement</li> </ul>"},{"location":"class_notes/CSE120/OS/#protection","title":"Protection","text":""},{"location":"class_notes/CSE120/OS/#cpu-modes","title":"CPU modes:","text":"<ul> <li>Kernal mode - can run all instructions</li> <li>User mode - can only run non-priviledged instructions</li> <li>Mode is indicated by a mode bit in a protected CPU control register</li> </ul>"},{"location":"class_notes/CSE120/OS/#priviledgd-instructions","title":"Priviledgd Instructions","text":"<p>a subset of instructions that can only run in kernal mode</p> <ul> <li>the CPU checks mode bit when priviledged instructions execute</li> <li>if the mode bit is set to user mode, the CPU will raise an exception</li> </ul>"},{"location":"class_notes/CSE120/OS/#priviledged-instructions-can-do","title":"Priviledged Instructions Can DO:","text":"<ul> <li>Directly access IO devices (disk, network, etc.)</li> <li>manipulate memory management (page tables, etc.)</li> <li>manipuate CPU protected control registers (mode bit, etc.), preventing user code from changing the mode bit</li> </ul> <p>HLT: halts the CPU</p>"},{"location":"class_notes/CSE120/OS/#memory-protection","title":"Memory Protection","text":"<p>prevents user code from accessing kernal memory, Seperation of user and kernal memory</p> <p>should programs trust OS?</p> <p>may/may not be trusted, but the OS should not trust the programs</p>"},{"location":"class_notes/CSE120/OS/#os-events","title":"OS Events","text":"<p>unnatural change in the flow of control</p> <ul> <li>stops current execution</li> <li>change mode, context or both</li> </ul> <p>OS defines handlers for these events, event handlers are executed in kernal mode after system booted, all entry to kernal occurs as result of an event</p> Os <p>OS itself is a big event handler</p> <p>OS only executes in response to an event</p>"},{"location":"class_notes/CSE120/OS/#interrupts","title":"interrupts","text":"<p>cause by external event</p>"},{"location":"class_notes/CSE120/OS/#exceptions","title":"exceptions","text":"<p>caused by program execution instructions - fault: eg: try execute priviledged instruction in user mode - system calls: eg: request kernal service</p> <p>events can be unxpected or deliberate</p>"},{"location":"class_notes/CSE142/","title":"CSE 142","text":"<p>CSE 142 - Introduction to Computer Architecture: A Software Perspective</p> <p>This course covers the operation, structure,  and programming interfaces of modern CPUs with an emphasis on exploiting processor features to improve software performance and efficiency. The topics covered in this course include performance, energy, x86 assembly, compiler optimizations,  pipelining, instruction-level parallelism, caches, memory-level parallelism, multi-threading, multi-core processors, and SIMD.</p>"},{"location":"class_notes/CSE142/cache/","title":"Cache","text":"<p>Created: February 13, 2025 4:03 AM Tags: Computer Hardware</p>  \ud83d\udca1  1 byte = 8 bits, 1 hex = 1 nibble = 4 bits, 1 KB = 1024 Bytes, 1 cycle = 0.3 nanoseconds    \ud83d\udca1  index of array is essentially offset of memory location, so they start with 0   <ul> <li>cache should store Frequently accessed, computationally expensive, or slow-to-retrieve data (e.g., database queries, API responses, session data).</li> <li>cache is stored in hashmap (key value pairs)</li> <li>cache is put in different storage based on speed and persistence</li> <li>check if key exists before query data from memory</li> <li>use LRU (least recently used), or LFU (least frequently used), or TTL (time to live) to manage cache</li> <li>if cache miss, retrieve from original mem location</li> </ul>"},{"location":"class_notes/CSE142/cache/#common-system-time","title":"common system time","text":"System Event Actual Latency Scaled Latency typical CPI One CPU cycle 0.3 ns 1 second 1 cycle Level 1 cache access 0.5 ns 2 seconds 4 cycles Level 2 cache access 2.8 ns 10 seconds 25 cycles Level 3 cache access 28 ns 2 minutes 50 cycles Main memory access (DDR DIMM) 100 ns 7 minutes 400 cycles SSD I/O 50\u2013150 \u03bcs 1.5\u20134 days 100000 cycles Rotational disk I/O 1\u201310 ms 1\u20139 months Internet packet: San Francisco to\u00a0Europe and back 150 ms ~10 years"},{"location":"class_notes/CSE142/cache/#classic-von-neuman-computer","title":"classic von neuman computer","text":""},{"location":"class_notes/CSE142/cache/#memory-wall","title":"memory wall","text":"<p>Memory performance improved at a much slower rate than processor performance. </p>"},{"location":"class_notes/CSE142/cache/#memory-access-is-expensive","title":"Memory access is expensive","text":"<ul> <li>if a non memory operation uses 1 cycle, a memory access opeeration would take ~ 500 cycles</li> </ul> <p>caching reduce CPI by access faster memories </p>"},{"location":"class_notes/CSE142/cache/#memory-techs","title":"memory techs","text":"<ol> <li>registers: 0.5 cycles (0.5ns)</li> <li>SRAM (static random access memory): 4-50 cycles (cache)</li> <li>DRAM: 400 cycles</li> <li>NAND (SSD): 100000 cycles</li> <li>hard disk</li> </ol>"},{"location":"class_notes/CSE142/cache/#calculate-for-cpi","title":"calculate for CPI:","text":"<p>for 100000 k iterations, there are:</p> <ol> <li>500 + 1 + 1 + 1 = 503 cycles per each iteration</li> </ol> <p>there are 100000 * 503 = 50300000 CPI  in total, and there are 3 + 3 + 2 + 4*100000 + 2 = 400010 dynamic instructions, so CPI = \\(50300000 / 400010 = 125.7\\) (or 503 / 4)</p>"},{"location":"class_notes/CSE142/cache/#optimize-memory-access-by-parallelism-amdahls-law","title":"optimize memory access by parallelism  (Amdahl\u2019s law)","text":"<ul> <li>we can only optimize the part x where it is parallezable. in this case,</li> </ul> <ul> <li>since k is large, we can assume iterations takes the entire execution time</li> <li>inside each iteration, <code>addl (%rax) %edx</code> is a memory access operation and its result edx is not used for later calculations, therefore it can be paralleized</li> <li>therefore,</li> </ul> \\[ x = \\frac{\\text{CPI of part that can be paralleized}}{\\text{CPI total per iteration}} = \\frac{500}{503} =  0.994 \\]  \ud83d\udca1  NOT SURE"},{"location":"class_notes/CSE142/cache/#memory-access-patterncache","title":"Memory access pattern/Cache","text":"<p>caching reduce CPI by access faster memories </p>"},{"location":"class_notes/CSE142/cache/#cache_1","title":"cache","text":"<ol> <li>faster than main memory</li> <li>usually made out of SRAM</li> </ol>  \ud83d\udca1  a hash map cache table stores the value in a virtual memory location    <p>3 layers of cache in modern computer </p> <p>with 64 bytes per cache line</p> <p>a cache is small, but close to processor</p> <p>memory access will try go to cache first </p> <p>cache hit</p> <p>cache miss - data is not in cache </p>"},{"location":"class_notes/CSE142/cache/#multi-level-cache","title":"multi level cache","text":"<p>last level cache - L3, LLC, the cache just before main cache </p> <p>L1 (cloest to processor), L2, l3</p>  \ud83d\udca1  cache only manipuate cache lines  where a cache line\u2019s address is always a multiple of cache line size,    \ud83d\udca1  purpose of memory hierachy:  1. provide fast memory access in most cases 2. hide the slowness of DRAM from application (by caching in SRAM ) 3, exploit locality to improve performance"},{"location":"class_notes/CSE142/cache/#memory-locality","title":"memory locality","text":"<p>based on: future memory access is predictable based on past access</p> <p>spatial locality - programs tend to access memory close in address to the data they have been accessed recently</p> <ul> <li>will divide an array into multiple data chunks, whenever a data in the chunk is loaded the entire chunk will be loaded</li> </ul> <p>temporal locality - programs tend to access same data repeatedly</p> <ul> <li>if run out of space, evict cache lines based on evication policy. new cache is always added</li> </ul>"},{"location":"class_notes/CSE142/cache/#calculate-cache-line","title":"calculate cache line","text":"<p>cache line:  smallest unit of data that can be transferred between main memory and cache, consists of multiple consecutive bytes (spatial locality)</p> <p>cache address: refer to location of memory block within a cache</p> \\[ \\text{cache line addr of A} = A - A \\text{ mod } 2^k \\] <ul> <li>where 2^k is the cache line size</li> </ul> <p></p> <p>unified cache : A cache that holds both instructions and data.</p>  \ud83d\udca1  since 1 byte = 2 hex, 16 bytes cache line = 4 ints   each cache line starts with address 0x\u20260, eg: 0x1000, 0x1020.    <p>example: </p> <p></p> <ul> <li>each element in the students array is unsigned int of 32 bits, or 4 bytes.</li> <li>therefore, \\(addr(student[i]) = 0x1004 + 4i\\)</li> <li>suppose the cache lines is 16 bytes, the cache line number is</li> </ul> \\[ \\text{cache line idx} = \\frac{\\text{memory address}}{\\text{cache line size}} \\] <ul> <li>since cache line is 16 bytes, therefore, there is 4 students object stored in each cache line,</li> <li>since the bit offset is 4 (2 ^ 4 == 16), therefore, the first 3 hex number defines a set,</li> <li>therefore, 0-2 (0x1004 - 0x100c), 3-6 (0x1010 - 0x101C), 7 - 10  is stored in the same cache line. since within each set, the first 3 hex digit is same.</li> <li>so answer is D</li> </ul>"},{"location":"class_notes/CSE142/cache/#cache-as-hashtable","title":"Cache as hashtable:","text":"<ul> <li>to ensure O(1) access and store time</li> <li>use tags to acces the cache line, and then use index bits to access a specific location.</li> </ul>  \ud83d\udca1  hashtable: group by : index bits \u2192 tag \u2192 memory contents (as k bit offset)  cache line : tag \u2192 index \u2192 offset   <p>A 64-bit address is divided into three parts:</p> <ol> <li>Offset (k-bit offset)<ul> <li>Determines which byte within a cache line to access.</li> <li>If the cache line size = 2^k bytes, the offset is k bits.</li> <li>Example: For 16-byte ( 2^4 ) cache lines, the offset = 4 bits.</li> </ul> </li> <li> <p>Index (Log(cache lines) index bits)</p> <p>The portion of an address that determines the possible locations of a cache line in the cache.</p> <ul> <li>Selects which cache line to use.</li> <li>If there are 2^n cache lines, the index requires n bits.</li> <li>Example: If the cache has 256 ( 2^8 ) cache lines, the index = 8 bits.</li> </ul> <p> \ud83d\udca1 <p>note: if use N-way set associative, the total number of indices = total number of associative sets = 2^n / N</p> <li> <p>Tag (Remaining bits)</p> <ul> <li>Used to differentiate memory blocks mapped to the same cache index.</li> <li>The CPU compares the tag of the requested address with the stored tag in the cache.</li> <li>Example: If 64-bit addresses, and offset + index uses 12 bits, then tag = 64 - 12 = 52 bits.</li> </ul> </li>  \ud83d\udca1  for example, a memory location `1100\u00a01010\u00a01111\u00a01110\u00a01011\u00a01010\u00a01011\u00a01110` stored at the cache set `11` = 3, at block by tag `1100\u00a01010\u00a01111\u00a01110\u00a01011\u00a01010\u00a01011`, and the `10` = 2 byte within that block"},{"location":"class_notes/CSE142/cache/#when-accessing-memory","title":"When accessing memory:","text":"<ol> <li>The Index selects the cache line.</li> <li>The Tag stored in that cache line is compared with the tag from the address.</li> <li>If the tags match \u2192 Cache Hit (data is retrieved from cache).If the tags don\u2019t match \u2192 Cache Miss (data is fetched from main memory).</li> </ol>  \ud83d\udca1  always loads the entire block tag represents to the cache."},{"location":"class_notes/CSE142/cache/#cache-metrics","title":"Cache Metrics","text":"\\[ \\text {hit rate} = \\frac{\\text{number of hits}}{\\text{number of accessses}} \\] \\[ \\text {miss rate} = \\frac{\\text{number of misses}}{\\text{number of accessses}} \\] <p>memory latency:  The time it takes to access a cache or main memory.</p> <p>Memory bandwidth: The number of bytes per second that can be transferred to or from a cache or memory.</p>"},{"location":"class_notes/CSE142/cache/#total-cpi-with-cache-misses","title":"total CPI with cache misses","text":"<p>example : </p> <p>In the absence of cache misses, processor takes an average of 2 cycle(s) to execute an instruction. If an L1 cache miss occurs, it takes an\u00a0additional\u00a018 cycles to access the L2.</p> <p>On this processor, for a particular program an L1 miss rate of 30%. 5% of the instructions access memory.</p> <p>What is the total CPI for the program?</p> \\[ 2 + 0.05*0.3 * 18=2.27 \\]"},{"location":"class_notes/CSE142/cache/#type-of-cache-misses","title":"type of cache misses","text":"<p>3Cs: Compulsory, Capacity, Conflict misses </p> <p>compulsory misses: Misses that occur because the cache has not been accessed before</p> <ul> <li>reduce cache line size (therefore fetches less data on miss, and does not exploit spatial locality) increase compulsory misses</li> </ul> <p>capacity misses:  Misses that occur because the cache is too small.</p> <ul> <li>Misses that are not compulsory and would still occur in fully-associative cache with the same number and size of cache lines</li> </ul> <p>conflict misses: Misses that occur because the indexing scheme and eviction policy caused a cache line to be evicted.</p> <ul> <li>Definition: Misses that are not compulsory and would not occur in a fully-associative cache the same number and size of cache lines</li> <li>increase cache line size (thus decrease the number of cache blocks and therefore same access pattern compete for fewer blocks) and  reduce associativity increase conflict misses</li> <li>fully associative eliminate all conflict misses, but much more expensive</li> </ul> <p>example</p> <p></p> <p></p> <ul> <li>0 : store. so we have index: 1, tag = 001,  compulsory miss</li> <li>1: load, so store cache line index = 0, tag = 000, compulsory miss</li> <li>2: store, index = 1, tag = 0, evict previous, compulsory (since it is first time access)</li> <li>3: store, index = 1, tag = 1, evict previous, capacity miss (even it is fully associatetive, we cannot avoid this miss)</li> <li>4: load, index = 1, tag = 1, cache hit</li> <li>5: index = 1, tag = 0, conflict miss, because if we have fully associative cache, then result from 2, 3 will all be cached and this will be a cache hit.</li> <li>6: index = 0, tag = 0, cache hit</li> <li>7: index = 0, tag = 1, compulsory miss, first time access this</li> </ul>"},{"location":"class_notes/CSE142/cache/#eviction","title":"eviction","text":"<p>occurs when we need to make space in cache for a new line </p> <p>eviction policy -  determines when to evict a cache line</p> <ul> <li>in this course, always make room for new cache line</li> <li>has impact on cache hit rate</li> </ul> <p>writeback - when a cache line is evicted and it is dirty (modified)</p> <ul> <li>cache write it back to next layer of memory hierachy  if dirty</li> <li>load the cache line from the next level of memory hierachy</li> </ul>"},{"location":"class_notes/CSE142/cache/#evication-policies","title":"Evication policies","text":""},{"location":"class_notes/CSE142/cache/#least-recently-used-lru","title":"least recently used (LRU):","text":"<ul> <li>the simple and good solution, evict the cache line that was used farthest in the past</li> <li>HOWEVER, it is hard to implement in hardware</li> </ul>"},{"location":"class_notes/CSE142/cache/#random-replacement-policy","title":"random replacement policy:","text":"<p>evicts a block chosen at random, regardless of usage. </p>"},{"location":"class_notes/CSE142/cache/#first-in-first-out-fifo-replacement-policy","title":"First In First Out (FIFO) replacement policy","text":"<p>policy evicts the block that was loaded into the cache the earliest. </p> <p>example: (suppose only temporal cache)</p> <p></p> <p>suppose the cache has 4 cache lines, 4 bytes per line, 32 bits addresss and 28 tag bits with 2 bit index and 2 bit offset</p> <p>so this will perform: </p> <p></p> <ol> <li>first try to load A[0] with tag (the higher address bits &gt; 4, in this case, 0-3 bits). so tag = 0001, then store the index bits (4 elements per cache line), so store 00 as 0. </li> <li>then cache miss B[0] with tag = 0011 and index bits = 00</li> <li>similarly, store A[1], B[1]. those are compulstory cache misses</li> <li>and second run all hits cache, therefore, hit rate is 50%</li> </ol> <p></p> <p>suppose there is only  2 lines in cache, then the cache hit rate is 0%, because in our case, when run out of cache lines, it will replace the first one inserted . cache miss due to capacity</p> <p>suppose with spatial locality and index bits = 1, offset bits = 3, and #cache lines = 4, this time each cache line should have 2 elements (A[0], B[0]), and they should loaded together. in this case,:</p> <ul> <li>miss when laod A[0], a[1],A[0] is loaded to cache</li> <li>miss when load B[0], B[0], B[1] is loaded to cache</li> <li>all later 6 access hits cache</li> <li>therefore hit rate is 75%</li> </ul> <p>same hit rate if loaded reversely (A[1]), since the entire block is loaded no matter what</p> <p>suppose k (the number of iterations) for both loop increase to 4, and index bits = 1, offset bits = 3. then the cache hit rate is </p> <ul> <li>cache miss A[0] \u2192 load A[0], A[1]</li> <li>cache miss B[1] \u2192 load B[0], B[1]</li> <li>cache hit A[1], B[1]</li> <li>cache miss A[2] \u2192 load A[2] A[3] in place of A[0] A[1]</li> <li>cache miss B[2] \u2192load B[2] B[3] in place of B[0] B[1]</li> <li>cache hit A[3], B[3]</li> <li>repeat again</li> </ul> <p>therefore, the hit rate is 50%</p> <p>more cache simulations</p>"},{"location":"class_notes/CSE142/cache/#cache-optimization-techniques","title":"Cache optimization Techniques","text":"<p>working set: data the programmer currently working on</p> <p>operational definition: smallest cache size for which there are few capacity misses</p>"},{"location":"class_notes/CSE142/cache/#direct-mapped-cache","title":"direct-mapped cache","text":"<p>each memory address maps to exactly one cache line. </p> <p>suppose 2 cache lines, 1 index bit, which of following has higher cache hit rate?</p> <p></p> <p>original: 50%</p> <p>A: 50%</p> <p>B: 75% , note: cache miss A[0], cache hit A[0], A[1], A[1]. repeat for all </p> <p>C: 50%</p> <p>loop interchage / renesting: </p> <p>the working set size: 16 bytes = 4 ints * 4 bytes per int +  4 ints * 4 bytes per int</p> <ul> <li>Although <code>A[]</code> and <code>B[]</code> together take 32 bytes in total, the loop splitting technique ensures that only part of them is in use at a time.</li> <li>The inner loop operates on <code>A[j]</code> and <code>B[j]</code>, keeping memory usage lower at each step.</li> <li>This reduces the active working set size at any given moment.</li> </ul> <p>loop renesting + loop spliitng:</p> <p></p> <p>we only need 1 cache line with 1 content (one content in a cache line)</p> <ol> <li>miss A[0], hit A[0]</li> <li>\u2026.</li> </ol> <p>so 50%hit rate</p>"},{"location":"class_notes/CSE142/cache/#cache-conflicts","title":"Cache conflicts","text":"<ul> <li>there is room for cache, but indexing is too restrictive, causing coliision in hash table</li> </ul> <ul> <li>in this case, since index biits is 1,  we only take the 4th bit for indexing. however, look at each case:<ul> <li>16 = <code>0001 0000</code> 4th bit is 0</li> <li>20 = <code>0001 0100</code> 4th  bit is 0</li> <li>\u2026</li> <li>notice all 4th bit is zero, therefore, all index is 0, and therefore only 1 content for each tag. so they can only use one cache line. therefore, all cache would have the same index, so all will stuck in the same cache line, and not use anything else. therefore, each cache is overwritten. so no cache hit</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/cache/#associativity-fully-associative-cache","title":"associativity, fully associative cache","text":"\ud83d\udca1  the fully associative cache does not use **index,** cache line can live anywhere. but the cache needs to search everywhere. associate cache is expensive since needs to compare all stored caches    <ul> <li>does not use index, but require to search all cache lines for matched tags</li> <li>allows to store multiple blocks in a cache set simutaneously</li> <li>fully associative cache eliminate conflict misses</li> </ul>"},{"location":"class_notes/CSE142/cache/#cache-with-associativity","title":"cache with associativity","text":"<ul> <li>suppose associativity  = 2, which means 2 of the cache lines do not use index but search all associated cache lines when tag is inquered</li> <li>therefore, first cacheline would store A[0], A[1], second would store B[0], B[1]. the first A[0], B[0] are compulsive misses, others are all cache hits</li> </ul>"},{"location":"class_notes/CSE142/cache/#hybrid-associative-cache-n-way-cache","title":"hybrid associative cache (N-way cache)","text":"<ul> <li>the index becomes the index of an associative set. an associated set have N cachelines, and they are associated.</li> <li>and  we can implement LRU evication within a set</li> </ul> <ul> <li>although index bit are all 0. all elements are cache in the first associative set.</li> </ul>"},{"location":"class_notes/CSE142/cache/#prefetching","title":"prefetching","text":"<ul> <li>bring data into the cache before its requested</li> <li><code>prefetch (%eax)</code></li> </ul>"},{"location":"class_notes/CSE142/cache/#cache-access-behavior-plot","title":"Cache access behavior plot","text":"<p> In this program we linearly access an array of memory of size 2^N\u00a0bytes (2^(N-2)\u00a0integers). We count the total time.</p> <p></p>"},{"location":"class_notes/CSE142/cache/#virtual-memory","title":"virtual memory","text":""},{"location":"class_notes/CSE142/cache/#address-space","title":"address space","text":"<p>each program runs as if it has its own memory. in 64 bits machine, address space should be 2^64 bytes. </p> <p>however, we can allocate more memory space than the entire physical space if we dont operation on them. this is virtualization </p>"},{"location":"class_notes/CSE142/cache/#virtualization","title":"virtualization","text":"<p>taking a physical resource (memory) and make it appear to behave differently than it does, usually more plentiful (eg: an program can have &gt;42TB address space on 16GB of memory) </p> <p>it takes limited physical memory and use it to build nice and easy address management spaces </p>"},{"location":"class_notes/CSE142/cache/#virtual-to-physical-mapping","title":"virtual to physical mapping","text":"<ul> <li>memory virtualization works on pages, ~4KB</li> <li>each  process has a page table to store virtual to physical mapping for address space<ul> <li>and permission to CRUD memory</li> </ul> </li> <li>programs can only operates on virtual addresses</li> </ul>  \ud83d\udca1  page table is a tree, with a depth fixed to 5 levels in latest intel CPUs. best case **memory translation** 20 cycles"},{"location":"class_notes/CSE142/cache/#page","title":"page","text":"<p>A page is a fixed-sized block of memory used in a virtual memory system. It is the basic unit of data transfer between RAM (physical memory) and storage (disk/SSD). Pages allow the operating system (OS) to efficiently manage memory and implement paging-based virtual memory.</p> <ul> <li>every process has a single page table</li> </ul>"},{"location":"class_notes/CSE142/cache/#translation-lookaside-buffer-tlb","title":"translation lookaside buffer (TLB)","text":"<ul> <li>The TLB is usually pretty small and highly (sometimes fully) associative.</li> <li>The TLB lookup has to complete before instruction can finish executing.<ul> <li>To enforce memory protection.</li> <li>Illegal accesses cause SIGSEGV</li> </ul> </li> <li>The hit rate needs to be high</li> </ul> <p>page: a unit of address translation </p>  \ud83d\udca1  typically, L1 cache has 100 entries of TLB, L2 has 1000 entries   <p></p> <ul> <li>TLB usually have high hits, .<ul> <li>hits are fast</li> <li>dont impact L1 time</li> <li>on miss, CPU asyncly performs a lookup in the OS manageed page table</li> <li>the table is in memory, so further cache misses are possible</li> </ul> </li> <li>if L1 is too small<ul> <li>add L2, L3 TLB (unrelated to L2, L3 cache)</li> <li>adopt even larger page sizes</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/cache/#page-offset","title":"page offset","text":"<p>specifies the exact byte location within a range </p> \\[ \\text{page offset bits} = \\log_2(\\text{page size bytes}) \\]"},{"location":"class_notes/CSE142/cache/#virtual-page-number-vpn","title":"virtual page number (VPN)","text":"<p>the index of a virtual page in a virtual address space The VPN is used in conjunction with the page table to look up the corresponding physical page. </p> \\[ VPN = \\text{total virtual page address bits} - \\text{page offset bits} \\] <ul> <li>in a 32 bit virtual address space with 4KB pages, VPN is 20bits, \\(= 32 - \\log_2(4*1024) = 32 - 12 = 20\\)</li> </ul>"},{"location":"class_notes/CSE142/cache/#physical-page-number-ppn","title":"physical page number (PPN)","text":"<p>The PPN (Physical Page Number) represents the index of a physical page in the physical memory.</p> <ul> <li>The OS and Memory Management Unit (MMU) use the VPN to find the corresponding PPN in the page table.</li> <li>The PPN + Page Offset forms the physical address.</li> <li>In a 32-bit physical address space with 4KB pages, the PPN consists of 20 bits.</li> </ul> \\[ PPN = \\text{physical address bits} - \\log_2(\\text{page size bytes}) \\]"},{"location":"class_notes/CSE142/cache/#convert-from-vpn-to-ppn","title":"convert from VPN to PPN","text":"\\[ \\text{virtual address} = VPN + \\text{page offset}\\\\ \\text{physical address} = PPN + \\text{page offset} \\] <ul> <li>PPN has a smaller range compare to VPN</li> <li>VPN is mapped to PPN, but not reverse versa. it is a one way map direction from VPN to PPN. there could be multiple VPN that points to same PPN</li> <li>the page offste of PPn and VPN is same</li> </ul> <p> the page offset bits = log_2 (4*1024) = 12, therefore, the lower 3 hex value is ignored (because page offset is ignored). then, since the remaiing <code>ce66e</code> \u2260 <code>ce66f</code> , they belong to different page </p> <p>Your program has a TLB miss rate of 0.03% and a cache miss rate of 0.1%. TLB misses add 12 cycles to the execution time of an instruction while cache misses add 11 cycles to the execution time of your program.</p> <p>You have to decide between optimizing to reduce cache misses or TLB misses. Which should you optimize for?</p> <p>(a)Cache misses</p> <p>(b)TLB misses</p> <p>A, because cache miss has higher miss rate? </p> <p>example: </p> <p> - since the array start address is 4kb-aligned. therefore, address intervals are: <code>0-4kb</code>, <code>4kb-8kb</code> - each element is 8 bytes, in a single page of <code>4kb</code>, therfore, there are 4096 / 8 = 512 elements. - therfore, element 1 is in second page, while element 2 is in first page, not in the same page</p>"},{"location":"class_notes/CSE142/cache/#summary","title":"summary","text":"<ul> <li>bigger cache<ul> <li>will generally have higher hit rates, lower cache miss</li> <li>let cache exploit spatial locality,</li> <li>\u2026 reduce compulsive misses</li> </ul> </li> <li>increase associativity<ul> <li>reduce conflict misses</li> </ul> </li> <li>prefetcher<ul> <li>reducer compulsory misses</li> </ul> </li> <li>ideally, cache should be large enough to fit all data in working set</li> </ul>"},{"location":"class_notes/CSE142/cache_aware_programming/","title":"Cache Aware programming","text":"<p>Created: March 4, 2025 11:50 AM Tags: Computer Hardware</p> <p>problems</p>"},{"location":"class_notes/CSE142/cache_aware_programming/#overview-of-cache-aware-programming","title":"Overview of Cache-Aware Programming","text":"<ul> <li>Key aspects include:<ul> <li>Organizing data structures.</li> <li>Structuring loops efficiently.</li> <li>Choosing optimal data structures and algorithms.</li> <li>Considering OS and library effects on caching.</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/cache_aware_programming/#compare-to-miss-rate-vs-mpi","title":"compare to miss rate vs MPI","text":"<p>Miss Rate Fallacy: A lower cache miss rate does not always mean better performance. </p> <p>Misses Per Instruction (MPI): More accurate than miss rate for understanding memory impact on performance.</p>"},{"location":"class_notes/CSE142/cache_aware_programming/#cold-start-effects","title":"cold start effects","text":"<p>Initial cache performance is lower before warm-up.</p> <ul> <li>It causes a temporary slowdown until caches or buffers are populated</li> <li>modify the code to reduce number of memory access instructions will also reduce MPI</li> </ul> <p>example: find the MPI in the section for each loop, suppose cache line size is 32 bytes </p> <p></p> <p>left: </p> <pre><code>movq -24(%rbq), %rqx             // cache miss, all -24(%rbq) stored\nmovl 8(%rax), %eax               // cache miss, 8(%rax) stored\nadddl %eax, -4(%rbq)             // cache hit, -4(%rbq)  is stored\nmovq -24(%rbq), %rax             // cache hit\nmovq (%rax), %rax                // cache hit\nmovq %rax, -24(%rbq)             // cache hit\ncmpq $0, -24(%rbq)               // cache hit\njne .L3                          // no memory access\n</code></pre> <p>therefore the MPI is 1/4</p> <p>right</p> <pre><code>addl 8(%rdi), %eax             //If the total number of instructions per node traversal is 8,\n                               // the cache miss ratio would be: 1/8\nmovq (%rdi), %rdi              //cache hit\ntestq ....                     //not memory access\nlne .....                      //not memory access \n</code></pre> <ul> <li>the cache miss rate per instruction is 1/8</li> </ul>"},{"location":"class_notes/CSE142/cache_aware_programming/#cache-aware-programming_1","title":"Cache aware programming","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#side-effects-of-cache-aware-programming","title":"side-effects of cache aware programming","text":"<ol> <li>side effects of cache-aware programming will make the code<ol> <li>longer</li> <li>more complicated</li> <li>harder to understand, modify, debug, and portable </li> </ol> </li> <li>if the code is working well, fix cache behavior is not first priority<ol> <li>unexploited performance optimization are not bugs</li> <li>only apply optimizations when we have evidence that the performance problem is due to memory behavior </li> </ol> </li> </ol>"},{"location":"class_notes/CSE142/cache_aware_programming/#spatial-locality-refresher","title":"spatial locality refresher","text":"<ul> <li>for cache: near: in the same cache line</li> <li> <p>for TLB (translational lookaside buffer):</p> <p>translate virtual address to physical address</p> <ul> <li>L1: 100 entries</li> <li>L2: 1000 entries</li> <li>therefore, it is important to determine How many cache lines contain a part of the data structure</li> </ul> </li> </ul> <p>example: for a 16 byte cache line, how big is <code>my_struct</code> in cachelines? </p> <pre><code>struct my_struct {\n    uint32_t foo[4]\n}\n</code></pre> <ul> <li>each object of <code>my_struct</code> would take 1 cache line, if it starts at the start of cache line</li> <li>if the start of my_struct is not the start of cache line, it would span over 2 cache lines</li> </ul> <p>example: order matters</p> <p></p> <ul> <li>64 byte cache line can store 64 / 4 = 16 ints,</li> <li>for version A<ul> <li>a, c in every struct would never be stored on the same cache line.</li> <li>therefore, the insturction <code>a[i].a += a[i].c</code> can be translate to <code>addl (%rax), 64(%rax)</code></li> <li>therefore, every cache access is a miss, needs to fetch twice per iteration</li> </ul> </li> <li>for version B, it is likely that a, c will be in the same cache line.<ul> <li>therefore, we only miss once when get a, and cache hits when c</li> </ul> </li> <li>on average, it should reduce the MPI by half</li> </ul>"},{"location":"class_notes/CSE142/cache_aware_programming/#alignment","title":"alignment","text":"<p>Alignment: Ensures data is naturally aligned in memory to prevent inefficient access. </p>  \ud83d\udca1  `sizeof(datatype)` = number of bytes this datatype holds   <p>aligned: <code>A % n == 0</code>, A is n byte aligned </p> <ul> <li>eg: <code>sizeof(char)==1</code>, therefore, they can be anywhere</li> <li>eg: <code>sizeof(uint32_t)==4</code>, therefore, should be at multiple of 4</li> </ul> <p>struct aligned: Final struct size is rounded to a multiple of the largest alignment (8 bytes).</p> <ul> <li>because<ul> <li>unaligned access are sometimes inefficient</li> </ul> </li> <li>unaligned access<ul> <li>in some ISA, may cause an interrupt</li> </ul> </li> <li>C, C++ require instances of basic types (int, float,\u2026) to be naturally aligned<ul> <li>or, the fields of structs is stored in the order they are declared</li> </ul> </li> </ul> <p>eg: what is the <code>sizeof(struct_8)</code></p> <pre><code>struct struct_8 {\n    uint8_t a;\n    uint64_t b;\n    uint8_t c;\n};\n</code></pre> <ul> <li>a, c is a size of 8 bits = 1 byte, so it can be placed anywhere</li> <li>b is a size of 64 bits = 8 bytes, so it must be aligned to 8-byte boundary</li> <li>therefore, the final memory layout is:</li> </ul> Field Size (bytes) Alignment Offset Notes <code>a</code> 1 1-byte aligned 0 Starts at 0 Padding 7 (To align <code>b</code> to 8-byte boundary) 1-7 Added by compiler <code>b</code> 8 8-byte aligned 8 Starts at offset 8 (must be aligned) <code>c</code> 1 1-byte aligned 16 Starts at offset 16 Padding 7 (To align struct to 8-byte boundary) 17-23 Ensures struct alignment"},{"location":"class_notes/CSE142/cache_aware_programming/#tensor","title":"Tensor <ul> <li>a 4D array, but we can set unused dimensions to 1 to reduce dimension</li> <li>in following case, suppose <code>z=1, b=1</code>, so it is 2D array</li> <li>therefore, index of every element is \\(index = y*(size.x)+ x\\)</li> </ul>  <ul> <li>therefore, all the adjancent x elements are consecutive within a y</li> <li>mostly in row-major<ul> <li>A(i, j) \u21d2 access A(y = i, x = j)</li> </ul> </li> </ul>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#temporal-locality-refresher","title":"temporal locality refresher  <p>exists when a program access same memory repeatedly in a short time</p> <ul> <li>recent accessed memory will be accessed soon</li> <li>programs tends to reuse the same memory location frequently within short period</li> </ul>  <p>requirements:</p> <ol> <li>reuse - must access A more than once</li> <li>the second access must be soon enough</li> <li>working set must fit the cache to exploit temporal locality <ol> <li>to shrink working set, we need to do loop optimization</li> </ol> </li> </ol>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#loop-optimization","title":"Loop optimization","text":"<p>change the order in which memory operations occur </p> <p>trade offs:</p> <ol> <li>benefits<ol> <li>reduce working set size</li> </ol> </li> <li>costs<ol> <li>increase loop overheads</li> </ol> </li> </ol>"},{"location":"class_notes/CSE142/cache_aware_programming/#performance-equation","title":"performance equation <ul> <li>loop optimization<ul> <li>reduce IC, CPI</li> <li>does NOT reduce cycle time (CT), because Loop optimizations do not change the hardware clock speed, so the cycle time remains the same.</li> </ul> </li> </ul>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#performance-counters","title":"performance counters  <p>hardware components in the CPU that track performance-related events </p>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#loop-fusion","title":"loop fusion <p>\u2705 Effect:</p> <ul> <li>Merges two loops with the same iteration space into one.</li> <li>Reduces cache misses by improving temporal locality (accessing the same data sooner).<ul> <li>and reduce loop overheads</li> </ul> </li> </ul> <p>\u26a0 Trade-offs:</p> <ul> <li>Not always applicable if loops have dependencies. (is the loop legal)</li> </ul>  <p>example: </p> <p></p> <ul> <li>each element would take 4 bytes, and therefore each cache line will store 64/4 = 16 elements.</li> <li>therefore, we will have 1 miss for each 16 memory access, those miss are compulsory</li> <li>if we translate each loop, there should be 5 instructions per loop block.</li> </ul> <p></p> <ul> <li>we only access memory once per 5 instructions, and only 1 out of 16 memory access cache miss</li> <li>therefore, the result MPI is<ul> <li>if the loop size is large enough that the cache will be completely filled (eg <code>size.x = 16384</code>)<ul> <li>result MPI is 1/5 * 1/16, there are capacity cache misses</li> </ul> </li> <li>if the loop size is small that all working set can be stored in the cache (eg <code>size.x  = 4096</code>)<ul> <li>result MPI is 1/10 * 1/16, all cache miss are compulsory</li> </ul> </li> </ul> </li> <li>if we use loop fusion, we have</li> </ul> <p></p> <ul> <li>we will reduce the MPI after fuse the loop, MPI of unfused / MPI of fused &gt; 1.xxxx ~ 2<ul> <li>note: we dont need to increment 2 <code>i</code> only 1 <code>i</code></li> </ul> </li> <li>however, this is affected by the cold start effects</li> </ul>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#loop-inversion-loop-renesting","title":"loop inversion (loop renesting) <p>\u2705 Effect:</p> <ul> <li>Changes loop nesting order to improve spatial locality by accessing memory in a more cache-friendly manner.</li> <li> <p>Reduces stride length, which is the gap between consecutive memory accesses.</p>  <p>The stride of a sequence of memory accesses is a consistent distance in bytes between the addresses. </p>  </li> </ul> <p>\u26a0 Trade-offs:</p> <ul> <li>Changes execution order, which might affect program correctness.</li> <li>legal renesting?</li> </ul>  <p>example:</p> <p></p> <ul> <li>NOTE: since sensor is directly mapped from b\u2192z\u2192y\u2192x, but here, each iteration, we try to get <code>a.get(i,j)</code>, and <code>a.get(i, j+1)</code>, which is <code>x = i, y = j -&gt; j+1</code>, therefore, the distance between each element access is entire size of x, or 1024 ints</li> </ul> \\[ 1024 * 4 = 4096 \\text{ bytes} \\] <ul> <li>what is the hit rate of a.get()?<ul> <li>since each memory access is 4096 byte apart so we will unable to exploit any spatial locaity, and there are only 32 kb / 64 bytes = 512 lines, and 512 * 64/4 &lt; 1024 * 1024, so cache capacity hits</li> <li>therefore, hit rate is 0%</li> </ul> </li> <li>if we invert the loop:</li> </ul> <p></p> <ul> <li>stride distance is 4 bytes</li> <li>miss rate is 1/16</li> </ul>  <p></p> <ul> <li> <p>the stride distance increase MPI when distance &lt; cache line size, since less memory will be cached when stride distance increases</p> <ul> <li>MPI increase rapidly at first. suppose cache line is 32 bytes and stride distance is 0 bytes, each element is an int 4 bytes, therefore, a cache line would store 32 / 4 = 8 ints at first, MPI = 1/8</li> <li>increase stride distance to 4 bytes, a cache line would store 32 / 8 = 4 ints, MPI = 1/4</li> <li>increase to 8 bytes, 32 / 12 ~= 2 in each cacheline MPI = 1/4</li> <li>\u2026</li> <li>when stride distancfe  &gt; 28 bytes, every try is a cache miss, therefore, MPI = 1 forever</li> </ul> \\[ MPI =(4 + SD) / 32 \\] <ul> <li>b</li> </ul> </li> </ul>  <p>Consider this\u00a0tensor&lt;&gt;\u00a0:</p> <pre><code>tensor_t&lt;float&gt; A(M, N);  // M rows, N columns\n</code></pre> <p>Each element is a float, which takes 4 bytes.</p> <p>Below is the definition of get():</p> <pre><code>float get(tensor_t&lt;float&gt; &amp;A, int i, int j)\n{\n    return A(i, j);\n}\n</code></pre>  <p>If we iterate by increasing index\u00a0j, what is the stride (in bytes) of memory accesses? Select the correct answer.</p>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#loop-fission","title":"loop fission <p>\u2705 Effect:</p> <ul> <li>Splits a large loop into multiple smaller loops, each handling a subset of operations.</li> <li>Can reduce working set size if different operations use different data.</li> </ul> <p>\u26a0 Trade-offs:</p> <ul> <li>Increases loop overhead (multiple loops mean more iterations).<ul> <li>but it may actually reduce the instruction count</li> </ul> </li> <li>Reduces cache pressure but may increase instruction overhead.</li> <li>does not help performance</li> </ul> <p></p>  <pre><code>for (int i = 0; i &lt; N; i++) {\n    A[i] = B[i] + C[i];  // First computation\n    D[i] = E[i] * F[i];  // Second computation\n}\n</code></pre> <p>After (Loop Fission Applied)</p> <pre><code>cpp\nCopyEdit\nfor (int i = 0; i &lt; N; i++) {\n    A[i] = B[i] + C[i];  // First loop\n}\n\nfor (int i = 0; i &lt; N; i++) {\n    D[i] = E[i] * F[i];  // Second loop\n}\n</code></pre>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#loop-tiling-blocking","title":"loop tiling (blocking) <p>\u2705 Effect:</p> <ul> <li>Breaks loops into smaller blocks that fit into cache, improving temporal locality.<ul> <li>if tiles are smaller than cache, misses should be eliminated.</li> <li>Optimizes data locality by accessing nearby elements before moving to new memory regions.<ul> <li>Improves cache efficiency by increasing data reuse within a block.</li> </ul> </li> </ul> </li> <li>reduce the working se size of loop<ul> <li>Reduces memory bandwidth usage by keeping smaller working sets in cache.</li> </ul> </li> </ul> <p>\u26a0 Trade-offs:</p> <ul> <li>Increases loop overhead due to extra inner loops.</li> <li>May not always provide benefits if tiles are not chosen optimally.</li> </ul>  <pre><code>// Before tiling (bad cache usage)\nfor (int i = 0; i &lt; N; i++)\n    for (int j = 0; j &lt; N; j++)\n        A[i][j] = B[i][j] + C[i][j];\n\n// After tiling (better cache usage)\nint T = 16;  // Tile size\nfor (int ii = 0; ii &lt; N; ii += T)\n    for (int jj = 0; jj &lt; N; jj += T)\n        for (int i = ii; i &lt; ii + T; i++)\n            for (int j = jj; j &lt; jj + T; j++)\n                A[i][j] = B[i][j] + C[i][j];\n</code></pre>  <p>example:</p> <p></p> <ul> <li>what happen to the number of cache misses?<ul> <li>cache misses remain constant</li> </ul> </li> <li>what happen to execution time<ul> <li>Might increase or decrease, depending on the value of block_size<ul> <li>depends on if the block size properly exploits spatial locality</li> </ul> </li> </ul> </li> <li>suppose there are 64 byte cache lines, what block_size will ensure you only get one miss per cache line?<ul> <li>the cache line size is 64 bytes, and each element is int (4 bytes). therefore, 16 elements per cache line</li> <li>therefore, the <code>block_size = 16</code> to fit in a tile</li> </ul> </li> </ul>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#data-structure","title":"Data structure","text":"<p>| Data   Structure | Insert | Delete | Search | Lookup | Memory Efficiency | Cache Misses | | --- | --- | --- | --- | --- | --- | --- | | std::vector&lt;&gt; (or <code>array</code>) | O(1) append | O(n) insert/delete | O(n) search | O(1) lookup | 100% (just an array) | O(n) for scan (1/8 per uint64_t),   O(log n) for bin-search | | std::list&lt;&gt; (Doubly Linked List) | O(1) append | O(1) insert/delete | O(n) search | N/A | Low (Each node \u2265 24 bytes) | O(n) misses for search (constant   factor 1 for uint64_t), O(1) for insertion | | std::map&lt;&gt;   / std::set&lt;&gt; (Red-Black Tree) | O(log n) | O(log n) | O(log n) | O(1) next largest element | ~64 bytes per node (low) | O(log n) misses (adds 1 miss per   tree size doubling) | | std::unordered_map&lt;&gt;   / std::unordered_set&lt;&gt; (Hash Table - Open Chaining) | O(1) | O(1) | O(1) | O(n) next largest | Similar to std::set&lt;&gt; (each   node \u2265 24 bytes) | O(1) misses lookup (bounded at 2   misses per query) |</p>"},{"location":"class_notes/CSE142/cache_aware_programming/#map","title":"map  <p>uses binary tree, red-black </p>  \\[ depth (D) = O(2\\log{(n)}) \\] <ul> <li>since this is a red-black binary tree, therefore, the D - 1 level map must be a complete binary tree</li> <li>therefore, \\(2^{D-1} - 1 \\le n \\le 2^D - 1\\)</li> </ul> \\[ 2^{D-1} - 1 \\le n \\le 2^D - 1 \\\\ D - 1 \\le \\log(n) \\le D \\\\ \\log(n) \\le D \\le 2\\log(n) \\]  <p>example: </p> <p>Consider a balanced binary tree of depth\u00a0D = 8\u00a0(i.e., each path from the root to a leaf contains exactly\u00a0D\u00a0nodes).</p> <ul> <li>Each node contains two pointers.</li> <li>Pointers are\u00a064\u00a0bits.</li> <li>Cache lines are\u00a032\u00a0bytes.</li> <li>The nodes of the tree are\u00a0distributed randomly across a very large region of memory\u00a0and they are cacheline-aligned.</li> </ul> <p>After constructing the tree, the program repeatedly\u00a0queries\u00a0it, and all searches terminate at a leaf.</p> <p>How large (in bytes) must the cache be to avoid nearly all capacity misses? (Assume the cacheline size and number of cachelines in the cache are both powers of 2).</p> <p>Hint:\u00a0How many nodes are there in a balanced tree of depth\u00a0D? Try some small numbers to figure it out.</p>  \ud83d\udca1  note: each map node has 2 pointers: left child and right child. therefore, the node struct size is 8 bytes   <ul> <li>the pointer is 64 bits = 8 bytes, a node has 2 childrne pointers, so the size of node is 16 bytes</li> <li>since The nodes of the tree are\u00a0distributed randomly across a very large region of memory\u00a0and they are cacheline-aligned. each node takes the entire cache line,</li> <li>so \\(2^8 * 32 = 8192\\) (since it wants power of 2)</li> </ul>","text":""},{"location":"class_notes/CSE142/cache_aware_programming/#struct","title":"struct <ul> <li>rearange the fields of a struct:<ul> <li>may reduce/increase size</li> <li>improve/worsen spatial locality</li> <li>DOES NOT  help ensure fields are naturally aligned</li> </ul> </li> </ul>  <p>example: </p> <p>For an object,\u00a0<code>a</code>,\u00a0<code>sizeof(a) == 32</code>\u00a0and\u00a0<code>&amp;a = 0x0000402c</code>. The cache line size is\u00a0<code>64</code>\u00a0bytes.</p> <p>Assuming the cache is initially empty, how many cache misses will occur if the program accesses every field of\u00a0<code>a</code>?</p> <p>since the cache line size is 64 bytes, therefore, each interval is 64/16 = 4, or <code>0x40</code>. therefore, the intervals include:</p> <ul> <li><code>0x00004000</code> - <code>0x0000403F</code>,</li> </ul> <p>and since <code>0x0000402C + 0x00000020</code> = <code>0x0000404C</code>, it would span over 2 intervals, therefore cause 2 cache misses. </p>  <p>Imagine that you have a doubly linked list with\u00a0X\u00a0elements in it, and the elements of the linked list are distributed randomly throughout a large region of memory. In addition to the pointers, each link in the list also contains a\u00a064-bit\u00a0integer.</p> <p>Your cache is fully associative, has\u00a0X\u00a0cache lines, and a cache line size of\u00a064 bytes. The system uses\u00a064-bit addresses. However, you find that each time you traverse the list, you incur\u00a02*X\u00a0cache misses.</p> <p>Which of the following changes would reduce the number of\u00a0capacity misses\u00a0to zero and reduce the number of\u00a0compulsory misses\u00a0by half? Select all that apply.</p> <p>Checkbox options[ ] </p> <p>(a)Reducing associativity[x] </p> <p>(b)Ensuring that the links were aligned to the beginning of a cache line[x] </p> <p>(c)Increasing cache line size (with a matching increase in overall cache size)[ ] </p> <p>(d)Using a write-through cache policy</p> <ul> <li>since the linked list has X elements, and each elemnt has 2 pointers and a 8 byte integer, therefore, the size of single element is 8 *3 = 24 bytes</li> <li>64 /24 result is not a complete integer. however, if distributed rnadomly in the memory, they are not aligned continously. \u2192 C is wrong because we cannot ensure they are continous</li> <li>reducing associativity does not reduce capacity misses, since it will only increase possibiliy of capacity misses</li> <li>if the element is not cache line aligned, then it is possible that we will encounter 2 cache misses perinstruction if the element is spanning 2 cache lines.</li> <li>A write-through policy only affects write operations, ensuring that every write propagates immediately to main memory.</li> <li>It does not reduce compulsory or capacity misses.</li> <li>Read accesses (which cause the traversal misses) remain unaffected by this change.</li> </ul>  <p></p> <p></p>","text":""},{"location":"class_notes/CSE142/parallelism/","title":"Parallelism","text":"<p>Created: March 15, 2025 1:58 PM Tags: Computer Hardware</p>"},{"location":"class_notes/CSE142/parallelism/#taxonomy-of-execution","title":"Taxonomy of Execution","text":"<ul> <li>SISD - single instruction, single data<ul> <li>eg:  add: sum 2 numbers and produce one result</li> </ul> </li> <li>MIMD - multiple instruction, multiple data<ul> <li>eg: multi-core</li> </ul> </li> <li>SIMD - single instruction, multiple data<ul> <li>eg: one instruction operates many values, like add 2 array elements element wise</li> <li>vector</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#threads","title":"threads","text":"<ul> <li>a thread is a flow of control<ul> <li>a program counter</li> <li>a stack</li> <li>a set of registers</li> </ul> </li> <li>all threads in one program share the same memory space<ul> <li>same data table</li> <li>and how they communicate with each other (Mutexes)</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#non-determism","title":"non-determism","text":"<ul> <li>paralell program results are not determined</li> <li>outcome depends on how the memory acccess are interleaved</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#multi-thread-pe","title":"multi thread PE","text":"<ul> <li>suppose all treads do equal work, there are T threads</li> <li>in the best case, each thread does 1/T of total work</li> </ul> \\[ ET_\\text{perThread} = IC_\\text{perThread}+CPI_\\text{perThread}+CT_\\text{perThread} \\] \\[ CT_\\text{perThread} \\approx CT \\] \\[ IC_\\text{perThread} = IC/T \\] \\[ CPI_\\text{perThread} = CPI \\] <ul> <li>CT may drop due to thermal effect, IC, CPI my increase due to sync. overhead</li> </ul> <p>$$</p> <p>ET_\\text{perThread} = ET / T \\ \\text{speed up} = T $$</p>"},{"location":"class_notes/CSE142/parallelism/#multi-processor-memory-system","title":"multi processor memory system","text":"<ul> <li>a complete processor contains<ul> <li>pipeline</li> <li>L1, L2 cache, they are private to each CPU</li> </ul> </li> <li>L3 cache is shared among CPUs</li> <li>comm latency is not uniform, varies based on the distance to other CPUs or memory banks</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#cache-line-invalidation","title":"Cache line invalidation","text":"<ul> <li>caused by a store in one of the CPU\u2019s L1,L2 cache, and needs to be synced across all cores</li> <li>caches are  coherent, managed by coherence protocal<ul> <li>all cores agree the same content of memory</li> <li>allows multiple headers, ensures there is only even one writer</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#coherence-misses","title":"coherence misses","text":"<p>A miss that occurs in a multiprocessor system when a cache block is invalidated because another processor modified the shared data.</p> <ul> <li>a cache miss is a coherence miss - if the cache line was there but it is invalidated</li> <li>suppose in CPU 1, x=10 is updated to 20, the CPU 2 where x = 10 will be invalidated and force fetch from the updated value</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#load-cache-miss","title":"load cache miss","text":"<ul> <li>can come from<ul> <li>The L1, L2cache on a different core.</li> <li>main memory</li> </ul> </li> </ul> <p>example: </p> <p></p> <ul> <li>here is what gonna happen, in one random run:</li> </ul> Event Coherence action CPU A cache CPU B cache memory A: t = array[0] A - compulsory cache miss [10,11,12,13] \u2026 [10,11,12,13] A:  array[0] = 11 A- cache hit [11,11,12,13] \u2026 [10,11,12,13] B: t = array[0] B - compulsory cache miss, copy from A [11,11,12,13] [11,11,12,13] [10,11,12,13] B:  array[0] = 12 B - cache hit, invalidate A invalidate [12,11,12,13] [10,11,12,13] A: t = array[1] A - coherence cache miss, copy from B [12,11,12,13] [12,11,12,13] [10,11,12,13] A:  array[1] = 12 A- cache hit, invalidate B [12,12,12,13] invalidate [10,11,12,13] B: t = array[1] B - coherance cache miss, copy from A [12,12,12,13] [12,12,12,13] [10,11,12,13] B:  array[1] = 13 B - cache hit, invalidate A invalidate [12,13,12,13] [10,11,12,13]"},{"location":"class_notes/CSE142/parallelism/#locks","title":"locks","text":"<p>lock make sure other threads cannot access the resources this thread already used. MAKE SURE TO <code>UNLOCK after</code></p> <ul> <li>we can define lock: <code>std:mutex lock</code></li> </ul> <p></p> <ul> <li>is the program deterministc?<ul> <li>NO, if B executes before A, result is different</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#cost-of-sharing-memory","title":"cost of sharing memory","text":"<ul> <li>copying and invaliding cache lines are expensive and takes a lot of time</li> <li>writing is a big deal, however, sharing READ ONLY is not a problem<ul> <li>sharing leads to coherence misses</li> </ul> </li> <li>we can do<ul> <li>per-thread data structure</li> <li>minimize lock acquire/release</li> </ul> </li> </ul> <p>example:</p> <p></p> <ul> <li>there will be compulsory misses, 1 per cache line get</li> <li>there will be a lot coherence misses, since lock is also shared across processors.<ul> <li>lock/unlock lock will invalidate others.</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#resolve-thread-data-conherence-misses","title":"resolve thread data conherence misses","text":"<ul> <li>give each thread own data</li> <li>merge at the end</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#false-sharing","title":"false sharing","text":"<p>example:</p> <p></p> <ul> <li>the program is determinstic</li> <li>although there is no sharing between the threads, there are still coherence misses, because we shared the cache lines but not the data</li> <li>law of sharing: all small, shared, more than very occcasionally updatd objects should be cache line aligned.</li> </ul>  \ud83d\udca1  locks are cache line aligned   to avoid false sharing, we need to make sure the data accessed by diferent cores are not in the same cache line, to do this, we might need to pad some junk data in between 2 non-shared data, for example:  <pre><code>volatile int non_share_a;\nint[32] junks\nvolatile int non_shared_b\n</code></pre> <p>example:</p> <p></p> <ul> <li>there is only 1 compulsory misses when each processor loads, therefore we can ignore it</li> <li>since bar fields are cached on the same cache line, a lot of coherence miss will occur</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#use-threads","title":"Use Threads","text":""},{"location":"class_notes/CSE142/parallelism/#openmp-open-multiprocessing","title":"OpenMP (open multiprocessing)","text":"<ul> <li>high level API for parallelism C and C++ code</li> <li>relatively easy</li> <li>most applicable to paralleizing simple loops</li> </ul>"},{"location":"class_notes/CSE142/parallelism/#a-openmp-requires-manual-thread-creation-and-synchronization","title":"\u274c (a) OpenMP requires manual thread creation and synchronization.","text":"<ul> <li>Incorrect because OpenMP automatically handles thread creation and workload distribution using simple directives (e.g., <code>#pragma omp parallel</code>).</li> <li>Unlike pthreads, where manual thread management is required.</li> <li><code>#pragma omp critical</code><ul> <li>To enforce mutual exclusion, allowing only one thread to execute the enclosed code at a time</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#pthreadsc-threads","title":"pthreads/C++ threads","text":"<ul> <li>low level API</li> <li>relatively hard</li> </ul> <p>example:</p> <p></p> <ul> <li>the working set size is 256 *8 = 2kb, this is a relatively small working set and can be cached without capacity miss on modern design. therefore, tilling does not help much</li> <li>function inlining does not help because <code>__attribute__((noinline))</code> is defined, which explicitly tell the complier not to inlineing the function</li> <li>loop unrolling would help, it reduce overhead</li> <li>there is no redudent execution so common sub-expression elimination dont help</li> <li> <p>splot loop does not help since it does not address memroy access patterns or memory redundency</p> </li> <li> <p>now, suppose we have N threads, and each process 1/N of data</p> <ul> <li> <p>suppose we use lock to protect increment of histogram[b]</p> <p></p> </li> <li> <p>what is speed up of 6 threads</p> <ul> <li>it would be even worse, because the lock transfer produce a lot of coherence misses, which essentially cause all the threads to wait for the other to update + additional coherence misses and validations</li> <li> <p>to improve</p> <ul> <li>currently, we have shared buckets, and shared lock for all threads</li> <li>we can improve a little by make a private lock for each bucket (therefore threads dont contests locks a lot (since lock occupy entire cacheline, so no coherence misses here)<ul> <li>but the threads still needs to wait to write into the historgram bucket <code>b</code>, this will still cause massive coherence misses</li> </ul> </li> <li> <p>use thread-private buckets: create 256 * 6  buckets, so each thread will have their own buckets for the histogram, and in the end we need to combine them.  ****in this case, define cache line like <code>buckets[bucket_num*thread_count + thread_id] = val</code> for each loop</p> <ul> <li>it has a good speedup, but the coherence miss still present.</li> <li> <p>since the buckets are created like following continous pattern, and each bucket is an int, multiple buckets would be on the same cacheline, and when one of them gets updated, a false sharing  would still occur to invalidate all other buckets in the same cache, even they are unrelated. this still cause coherence misses</p> <p></p> </li> </ul> </li> <li> <p>to fix it, we can use thread-private buckets and let each bucket occupy entire cache line. in this case, define buckets as <code>buckets[thread_id * 256 + bucket_number ] = val</code> for each loop. therefore, instead of same bucket number from different threads are adjacent to each other, we make same threads buckets adjacent to each other. therefore, we avoid most coherence misses, there are still some coherence misses possible in the cache line that contains buckets from differnt threads.</p> <ul> <li> <p>and we also eliminated all locks!</p> <p></p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p></p> <p>original</p> <p></p> <p>private lock for each shared bucket </p> <p></p> <p>private bucket for each thread, grouped by bucket numbers</p> <p></p> <p>private buckets for each thread,  grouped by threads</p>"},{"location":"class_notes/CSE142/parallelism/#summary-parallelism-speedup","title":"summary parallelism speedup","text":"<ul> <li>we need to do some aggressive optimization that complier cannot do for us<ul> <li>eliminate (or reduce) lock contention</li> <li>eliminate false sharing</li> <li>eliminate true sharing</li> <li>redesign our data structure to do above</li> </ul> </li> <li>private caches<ul> <li>most modern processors<ul> <li>L1, L2  are private caches to each core<ul> <li>provides faster access to frequently used data for single core</li> </ul> </li> <li>when the cached values gets update in the other core, coherence manager invalidates cache in this core so the next time this core needs to access the value, it will copy from the updated cache to ensure memory consistency</li> <li>in addition, registers files are private to each core<ul> <li>registers can not be shared</li> </ul> </li> </ul> </li> </ul> </li> <li>shared caches<ul> <li>LLC (l3) is typicall shared amongst cores to help access data without going to memory</li> <li>shared cache access time is different for different cores</li> </ul> </li> <li>coherence<ul> <li>L1, L2 cache value is same across</li> <li>L3, DRAM, and other further storage (DIMM) value is same</li> <li>cached value can be different in the above 2 groups</li> </ul> </li> <li>threads<ul> <li>a thread has their own register file (incl. PC), L1, L2 cache</li> <li>to make use of multi processors</li> <li>share the same address space with other threads in the same program</li> </ul> </li> </ul>  \ud83d\udca1  SMT - simultaneous multithreading"},{"location":"class_notes/CSE142/parallelism/#vector-and-simd","title":"Vector, and SIMD","text":"<ul> <li>where a single isntruction operates on multiple data values of same type</li> <li>for example, adding 2 vector element wise</li> </ul> <ul> <li>CPI = 1, cycles/addition = 0,25<ul> <li>floating point, huge arrays<ul> <li>machine learning</li> <li>scientifc computing</li> </ul> </li> <li>floating point, short arrays<ul> <li>3D graphics</li> </ul> </li> <li>media (mixed float/int)</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/parallelism/#arithmetic-instructions","title":"\ud83d\udd39 Arithmetic Instructions","text":"Instruction Description Width <code>ADDPS</code> Add packed single-precision floats 128-bit <code>ADDPD</code> Add packed double-precision floats 128-bit <code>VADDPS</code> Add packed single-precision floats 256/512-bit <code>MULPS</code> Multiply packed single-precision floats 128-bit <code>VMULPS</code> Multiply packed single-precision floats 256/512-bit <code>SUBPS</code> Subtract packed floats 128-bit <code>VSUBPS</code> Subtract packed floats (AVX) 256/512-bit <code>DIVPS</code> Divide packed single-precision floats 128-bit"},{"location":"class_notes/CSE142/parallelism/#logical-instructions","title":"\ud83d\udd39 Logical Instructions","text":"Instruction Description Width <code>ANDPS</code> Bitwise AND of packed floats 128-bit <code>VANDPS</code> Bitwise AND of packed floats (AVX) 256/512-bit <code>ORPS</code> Bitwise OR 128-bit <code>XORPS</code> Bitwise XOR 128-bit <code>VORPS</code> Bitwise OR (AVX) 256/512-bit <code>VXORPS</code> Bitwise XOR (AVX) 256/512-bit"},{"location":"class_notes/CSE142/parallelism/#data-movement","title":"\ud83d\udd39 Data Movement","text":"Instruction Description Width <code>MOVAPS</code> Move aligned packed floats 128-bit <code>MOVUPS</code> Move unaligned packed floats 128-bit <code>VMOVAPS</code> Move aligned packed floats (AVX) 256/512-bit <code>VMOVUPS</code> Move unaligned packed floats (AVX) 256/512-bit"},{"location":"class_notes/CSE142/parallelism/#permute-shuffle","title":"\ud83d\udd39 Permute / Shuffle","text":"Instruction Description Width <code>SHUFPS</code> Shuffle packed floats 128-bit <code>VSHUFPS</code> Shuffle packed floats (AVX) 256/512-bit <code>PERMPS</code> Permute single-precision floats (AVX2) 256-bit <code>VPERMQ</code> Permute 64-bit elements (AVX2/AVX-512) 256/512-bit"},{"location":"class_notes/CSE142/parallelism/#comparison-masking","title":"\ud83d\udd39 Comparison / Masking","text":"Instruction Description Width <code>CMPPS</code> Compare packed floats 128-bit <code>VCMPPS</code> Compare packed floats (AVX) 256/512-bit <code>BLENDPS</code> Blend packed floats based on mask 128-bit <code>VBLENDVPS</code> Variable blend using mask (AVX) 256-bit"},{"location":"class_notes/CSE142/pipeline/","title":"Pipeline","text":"<p>Created: March 14, 2025 8:33 PM Tags: Computer Hardware</p>"},{"location":"class_notes/CSE142/pipeline/#iem-instruction-execution-machine-02","title":"IEM - instruction Execution Machine 0.2","text":"<p>each instruction goes 1 each stage per cycle, </p> <p>each stage take 1 clock cycle, so single instruction executes 6 cycles </p> <p>only 1 instruction is executing at any time </p>  \ud83d\udca1  recall: IC - set up programming, CPI - this design: 6/good design = 1, CT - large in this design   <p>example: what is CPI in this design:</p> <p></p> <ul> <li>the CPI is 6, the cycles: fetch \u2192 decode \u2192 RF read \u2192 execution/memory \u2192 RF write \u2192 update PC</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#pipeline-cpu","title":"pipeline CPU","text":"<ul> <li>the CPU pipeline alllows each cycle to handle different instruction in order, so concurrently we can execute 6 instruction in order, just like assembly line</li> <li>however, it is NOT impacting performance,<ul> <li>pipelined CPI: 1 (from 0.2 reduce from 6)</li> <li>cycles to execute a single instruction remains roughly constant</li> <li>required:<ul> <li>CPU initiate new instruction each cycle</li> <li>CPU completes an instruction in each cycle</li> </ul> </li> </ul> </li> </ul>  \ud83d\udca1  If the prediction was incorrect, the processor squashes instructions on the mispredicted path."},{"location":"class_notes/CSE142/pipeline/#hazards","title":"hazards","text":"<ul> <li>when processor dont have something needs to process instruction</li> <li> <p>control hazards: fetch stage does not know which instruction to fetch</p> <p>A hazard that occurs when the pipeline needs to determine the correct instruction to fetch after a branch, or ran into pipeline stall (bubble) </p> <p>pipeline stall: A temporary delay in the pipeline execution flow (e.g., due to control hazards), where no new instructions are issued in some cycles</p> <ul> <li>possible causes:<ul> <li>ran into a loop</li> <li>if statement checks for unlikely error conditions</li> <li>unconditional branches</li> </ul> </li> <li>solution: speculative execution,<ul> <li>processor guess (speculate) which branch will go</li> <li>no more hazards</li> <li>how to guess?<ul> <li>guess backward branch? (since loop goes backwards)</li> <li>cached prediction<ul> <li> <p>create a map with key = 00,01,10,10, value is boolean (default false)</p> key was_last_branch_taken 00 true 01 false - suppose at branch predictor, we want to predict if program will go to<code>addr</code> - take the first 2 bits of <code>addr</code>, and look up at the table. if <code>was_last_branch_taken == true</code> , predict to goto that branch - on UPDATE PC stage, get the prediction result and update the table - how to rollback if guess is wrong - processor squashes it when branch resolves - discard/rollback/suppress register/memory updates - called flushing the pipeline - CPU flushes </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>data hazards: execution stage does not have operand it requires</p> <p>A situation where an instruction depends on the result of a previous instruction that has not yet completed.</p> <ul> <li>for example, if we have the following instruction:</li> </ul> <pre><code>```c\naddl %esi, %edi //fetch @ 1, write to register file @ 5\naddl %edx, %edi //fetch @ 2, needs to read from RF @ 4, but its not available\n```\n</code></pre> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#iem-06","title":"IEM 0.6","text":"<ul> <li>a branch detector is used to make predictions based on prior branch outcomes<ul> <li>fetch: predecode and predict next PC</li> <li>update PC: detect mispredictions</li> </ul> </li> <li>Update PC stage must correct if wrong, misprediciton recovery</li> <li> <p>modern predictors:</p> <ul> <li>use bigger table</li> <li>dont change prediction after one misinterpretation</li> <li>account for history of branch behavior, and global pattern of branch behavior\u2026</li> </ul> <p></p> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#iem-08","title":"IEM - 0.8","text":"<ul> <li>x86 pipelines DONT actually execute x86 instructions, they translate x86 instruction set into micro-ops or uOps (IEM-0.8)<ul> <li>CPI = 1, CT = low, supports x86</li> <li>uOps are simple,<ul> <li>small number of arithematic ops</li> <li>0 or 1 mem access</li> <li>0-2 reads, 0-1 write</li> </ul> </li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#iem-10","title":"IEM 1.0","text":"<ul> <li>CPI &lt; 1 means more than 1 instruction finishes executing per cycle<ul> <li>execute instructions ASAP using instruction level paralleism</li> <li>processor utilize ILP using out of order execution (OOO)</li> </ul> </li> <li>OOO solve several problems at once<ul> <li>reduce CPI</li> <li>handles data depencies, long-latency instructions</li> </ul> </li> <li>OOO concepts<ul> <li>register dependencies</li> <li>register renaming</li> <li>OOO issue</li> </ul> </li> </ul> <p>example: which pair of uOPs can execute in reverse order?</p> <p></p> <ul> <li>E</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#data-dependencies","title":"data dependencies","text":"<ul> <li> <p>it constrain the order of instruction</p> <ul> <li>read-after-write (RAW)<ul> <li>True dependency</li> <li>we want processor only constrained by True dependency (RAW)</li> </ul> </li> <li> <p>write-after-write (WAW)</p> <ul> <li>False dependecy</li> <li>when both instruction write to the same register</li> </ul> <pre><code>mov 2, %edx\nmov 4, %edx\n</code></pre> <ul> <li>we can make them write to different register, so we can perform OOO</li> </ul> <pre><code>mov 4, %ecx //order swiched\nmov 2, %edx //final value in edx is same\n</code></pre> </li> <li> <p>write-after-read (WAR)</p> <ul> <li>false dependency</li> </ul> <pre><code>mov %edx, %eax //read\nmov 4, %edx //write\nadd %eax, %eax \n</code></pre> <ul> <li>similalry, we can write to different registers to perform OOO</li> </ul> <pre><code>mov 4, %ecx //after order switch\nmov %edx, %eax\nadd %eax, %eax //final value in eax is same\n</code></pre> </li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#dependencies-and-dataflow-graphs","title":"dependencies and dataflow graphs","text":"<ol> <li>(a) They do not include branches. \u274c<ul> <li>False. Dependence graphs can include branches since branches affect control dependencies.</li> </ul> </li> <li>(b) Nodes represent instructions. \u2705<ul> <li>True. In a dependence graph, each node corresponds to an instruction in the program.</li> </ul> </li> <li>(c) Fewer edges in the graph generally means more ILP. \u2705<ul> <li>True. Fewer dependencies allow more instructions to execute in parallel, increasing Instruction-Level Parallelism (ILP).</li> </ul> </li> <li>(d) The edges include memory and register values. . \u274c<ul> <li>False. Edges represent dependencies, including both register and memory dependencies (RAW, WAR, WAW), but not values</li> </ul> </li> <li>(e) Only the compiler can construct a dependence graph. \u274c<ul> <li>False. The hardware (e.g., Tomasulo\u2019s algorithm, dynamic schedulers) and compilers can both construct and use dependence graphs.</li> </ul> </li> <li>(f) The critical path through the graph sets the minimum number of cycles it takes to execute the instructions. \u2705<ul> <li>True. The longest dependency chain (critical path) determines the minimum execution time in cycles.</li> </ul> </li> <li>(g) Edges represent a requirement that one instruction execute before another. \u2705<ul> <li>True. Edges denote data dependencies (RAW), anti-dependencies (WAR), and output dependencies (WAW) that constrain instruction execution order.</li> </ul> </li> </ol> <ul> <li>dependency graph</li> <li>critical path is the longest path, assume every node take 1 cycle<ul> <li>the length of critical path is 7, 9\u2192 12 \u2192 13 \u2192 14 \u2192 15 \u2192  16 \u2192 17</li> </ul> </li> <li> <p>the average CPI for the dataflow graph is</p> \\[ CPI = \\frac{\\text{critical path number of cycles}}{\\text{number of instructions}} \\] </li> <li> <p>therefore, CPI = 7 / 9</p> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#instruction-level-parallelism-ilp","title":"instruction level parallelism (ILP)","text":"<ul> <li>exists when multiple instructions can be executed at the same time</li> <li>instructions per cycle (IPC) = 1 / CPI</li> <li>the x86 does not have many registers to use, therefore, we need to<ul> <li>rename some of the architecture register to physical registers to use</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#register-alias-table-rat","title":"register alias table (RAT)","text":"<ul> <li>rename the architectural registers (r9, di, si) to physical registers (pr19, does not actually exist)</li> <li>architectural registers:<ul> <li>A register explicitly defined by the ISA and visible to the programmer through the instruction set.</li> </ul> </li> <li>physical registers:<ul> <li>A register in the CPU that stores instruction results, including uncommitted ones.</li> </ul> </li> </ul> <ol> <li>r9 is renamed to pr19, read from the table, <code>di</code> is mapped to <code>pr16</code>. therefore:</li> <li>suppose we rename the output to <code>pr20</code>, so <code>r9</code> is update to <code>pr20</code>. look up latest di and r9 before this row from the table. <code>di</code> maps to <code>pr16</code>, and <code>r9</code> maps to <code>pr19</code> </li> <li>suppose renam the output to <code>pr21</code>, it maps <code>ax</code> to <code>pr21</code></li> <li> <p>\u2026.</p> Code Inputs Outputs Renamed Inputs Renamed Outputs %ax %di %r9 <code>mov %edi, %r9d</code> <code>di</code> <code>r9</code> <code>pr16</code> <code>pr19</code> pr0 pr16 pr19 imull %esi, %r9d <code>di r9</code> <code>r9</code> <code>pr16 pr19</code> <code>pr20</code> pr20 <code>mov %r9d, %eax</code> r9 ax <code>pr20</code> <code>pr21</code> <code>pr21</code> mov %edx, %r9d dx r9 <code>pr3</code> <code>pr22</code> pr22 imull %edx, %r9d dx r9 <code>r9</code> <code>pr3</code> <code>pr22</code> <code>pr23</code> pr23 subl %r9d, %eax r9 ax ax <code>pr23</code> <code>pr21</code> <code>pr24</code> pr24 </li> <li> <p>therefore, we simply the datagraph:</p> </li> </ol> <p></p> <ul> <li>now it length of critical path = 4, CPI = 4/6, ILP = 6/4</li> </ul> <p>example: </p> <p></p> code inputs output rename input rename output %r0 %r1 <code>sub r1 = r1,r0</code> r1, r0 r1 r1, r0 r2 r2 <code>div r0 = r0, r1</code> r0, r1 r0 r0, r2 r3 r3 <code>add r0 = r1,r0</code> r1, r0 r0 r2,r3 r4 r4 <code>mul r1 = r0,r0</code> r0 r1 r4 r5 r5 <code>add r1 = r1, r0</code> r1, r0 r1 r5, r4 r6 r6"},{"location":"class_notes/CSE142/pipeline/#exploit-ilp","title":"exploit ILP","text":""},{"location":"class_notes/CSE142/pipeline/#reorder-buffer-rob","title":"reorder buffer (ROB)","text":"<ul> <li>CPU fetches many instructions keeps them in a reorder buffer</li> <li>each cycle CPU selects instructions with no remaining dependencies and exec them<ul> <li>since it can execute multiple instructions, CPI can be &lt; 1</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#tomasulos-algorithm","title":"Tomasulo\u2019s Algorithm","text":"<p>enables out-of-order execution by allowing instructions to execute as soon as their operands become available. </p> <p>Reservation Stations: Each instruction (uOp) is assigned a reservation station, which holds necessary operands and waits for execution.</p> <ul> <li>Issuing Instructions:<ul> <li>Instructions can issue when all input operands are available.</li> <li>If operands are missing, they are fetched from either:<ul> <li>The physical register file before entering the scheduler.</li> <li>Broadcasted by an ALU (arthematic logic unit) after a previous instruction completes.</li> </ul> </li> </ul> </li> <li>Execution Readiness: The table lists reservation stations and shows whether each instruction is ready to issue.</li> </ul> <p></p> <ul> <li>Reservation Station #0 (add) is ready to issue because:<ul> <li>All input operands (pr10 and pr11) are available.</li> <li>It will broadcast pr21\u2019s value (0x1aa34) after execution.</li> </ul> </li> <li>Station #1 (sub) is waiting on pr21 from station #0.</li> <li>Station #2 (mult) is also waiting on pr21.</li> <li>Station #3 (xor) is waiting on pr31, which is not available.</li> </ul> <p>execution:</p> <ul> <li>Station #0 issues and completes execution.</li> <li>It broadcasts pr21 = 0x1aa34.</li> <li>Stations #1 and #2 receive pr21\u2019s value and become ready to issue.</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#modern-iems","title":"modern IEMs","text":""},{"location":"class_notes/CSE142/pipeline/#effective-instruction-latency","title":"Effective instruction Latency","text":"<p>It is the time an instruction adds to the critical path, considering dependencies. </p> <p>example:</p> <p></p> <ul> <li>1 cycle</li> <li>if it is not in the critical path, then the effective latency is 0</li> <li>if on CP, then latency is how many cycles it adds</li> </ul> <p></p> <ul> <li>the CP is mul \u2192 add r3 \u2192 add r4 \u2192 div \u2192 add r7<ul> <li>x is not in the critical path, therefore mem latency is 0</li> <li>y is in the critical path, therefore mem latency is 1</li> </ul> </li> </ul>"},{"location":"class_notes/CSE142/pipeline/#effective-memory-latency","title":"effective memory latency","text":"<ul> <li>long, at least 4 cycles, 100s of cycles max</li> <li>cache miss often on CP</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#latency-hiding","title":"latency hiding","text":"<ul> <li> <p>(b)Parallel execution helps hide latency by allowing multiple instructions to proceed simultaneously.</p> </li> <li> <p> </p> <p>(c)It helps reduce the visible impact of instruction or memory latency by keeping the processor busy.</p> <p>Out of order execution allows the pipeline to hide the latency of some instructions.</p> </li> <li> <p>when there are 2 memory loads, one of them is hided from the CP</p> </li> </ul> <p></p> <ul> <li> <ol> <li>the number of cycles is 4 * 5 = 20, the number of instructions is 10, therefore, average memroy latency is 20 / 10 = 2</li> </ol> </li> <li>what happen if the right all cache misses (<code>movq (%rsi), %rsi</code>) and take 12 cycles per load?<ul> <li>we call the misses hides the latency of hits, the hits are free</li> </ul> </li> </ul> <p></p> <ul> <li>C, the CP is 4 cycles, it hides the path</li> </ul> <p>example: find the max speedup</p> <p></p> <ul> <li>we can speed up memory CPI to 1 cycle at max, so<ul> <li>improved memory time: 5 / 2.1 = 2.38</li> <li>new total time = (2.38 + 65) * 1.07 = 72.096</li> <li>the speed up = 70 / 72.096 = 0.9709</li> </ul> </li> <li>we already shown that even under the most optimal memory time, it would worsen the total execution time. since no matter how many load/store units we have we can never reach the optimal CPI, we can never get a better speed up.</li> <li>therefore, opt # is 1, and speed up = 0,97</li> </ul> <p></p>"},{"location":"class_notes/CSE142/pipeline/#_1","title":"pipelining","text":""},{"location":"class_notes/CSE142/pipeline/#1-fetch","title":"1. fetch","text":"<ul> <li>retrieve instruction forom memory using program counter (PC)</li> <li>predict next PC</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#2-decode","title":"2. decode","text":"<ul> <li>examine instruction and determine what it does</li> <li>decompose instructions in to uOPs</li> <li>enqueue resulting Uops in the decode queue</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#3-rf-read","title":"3. RF read","text":"<ul> <li>the register operands for the instruction</li> <li>dequeue an uOp from the decode queue, and read register Operands for the instruction</li> </ul>  \ud83d\udca1  data forwarding: It reduces stalls by passing an instruction's result directly to a dependent instruction in the next pipeline stage, bypassing the register file."},{"location":"class_notes/CSE142/pipeline/#4-executionmemory","title":"4. execution/memory","text":"<ul> <li>perform  Uops arithematic operations and loads/stores</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#5-rf-write","title":"5. RF write","text":"<ul> <li>update the register file</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#6-compute-next-pc-update-pc","title":"6. compute next PC \u2014 update PC","text":"<ul> <li>if instruction is a branch, set PC according to outcome</li> <li>if not, increment PC to point to next instruction</li> <li>going back to fetch</li> </ul>"},{"location":"class_notes/CSE142/pipeline/#problems-with-x86-instructional-set","title":"problems with x86 instructional set","text":"<ul> <li>The amount of work per instruction varies.</li> <li>Variable length instructions make it hard to know where the next instruction starts.</li> </ul>"},{"location":"leetcode/","title":"Leetcode questions","text":""},{"location":"leetcode/#sorting","title":"Sorting","text":""},{"location":"leetcode/#basic","title":"Basic","text":"<ul> <li>sort list</li> </ul>"},{"location":"leetcode/sorting/sort_list/","title":"Sort List","text":"<p>leetcode</p> <p>!!! note \"Review\" this is a basic sorting problem, we can use merge sort to solve this problem.</p> <p>??? info \"Added Date\" 2021-09-30</p>"},{"location":"leetcode/sorting/sort_list/#problem","title":"Problem","text":"<p>Given the head of a linked list, return the list after sorting it in ascending order. </p> <p>Input: head = [4,2,1,3] Output: [1,2,3,4]</p>"},{"location":"leetcode/sorting/sort_list/#workflow","title":"workflow","text":"<pre><code>graph LR\n    A[Start] --&gt; B{head == null}\n    B --&gt;|yes| C[return head]\n    B --&gt;|no| D[create a list]\n    D --&gt; E[add all elements to the list]\n    E --&gt; H[connect the elements]\n    H --&gt; I[End]</code></pre>"},{"location":"leetcode/sorting/sort_list/#solution","title":"solution","text":"<p>=== \"java\" ```java title=\"Merge Sort\" linenums=\"1\" hl_lines=\"1\" public ListNode sortList(ListNode head) { if (head == null) return head;</p> <pre><code>        List&lt;ListNode&gt; list = new ArrayList&lt;&gt;();\n        while (head != null) {\n            list.add(head);\n            head = head.next;\n        }\n\n        Collections.sort(list, (a, b) -&gt; a.val - b.val);\n\n        for (int i = 1; i &lt; list.size(); i++)\n            list.get(i - 1).next = list.get(i);\n\n        list.get(list.size() - 1).next = null;\n\n        return list.get(0);\n    }\n```\n</code></pre> js <p><code>javascript title=\"Merge Sort\" linenums=\"1\" hl_lines=\"1\" /** function sortList(head: ListNode | null): ListNode | null { // merge sort if (!head) return null; //divide if (!head.next) return head; let fast = head, slow = head; while (fast &amp;&amp; fast.next &amp;&amp; fast.next.next) { slow = slow.next; fast = fast.next.next; } const oh2 = slow.next; slow.next = null; let l1 = sortList(head); let l2 = sortList(oh2); let dummy = new ListNode(1); let cur = dummy; while (l1 &amp;&amp; l2) { if (l1.val &lt; l2.val) { cur.next = l1; l1 = l1.next; } else { cur.next = l2; l2 = l2.next; } cur = cur.next } while (l1) { cur.next = l1; l1 = l1.next; cur = cur.next } while (l2) { cur.next = l2; l2 = l2.next; cur = cur.next } return dummy.next; };</code></p>"},{"location":"notes/","title":"Notes!","text":""},{"location":"notes/#common-tricks-and-tips","title":"common tricks and tips","text":""},{"location":"notes/#1-manually-change-the-image-url-when-move-to-another-folder","title":"1. manually change the image url when move to another folder","text":"ProblemSolutionExplanation <p>In this example, I moved the images from <code>/</code> to <code>/cache</code> folder, now I need to update all images url in <code>md</code></p> <p>for example: <code>![image](image.jpg)</code> to <code>![image](cache/image.jpg)</code></p> <p>use built-in <code>find</code> and <code>replace</code> feature in vscode to do this.</p> <p>using regex expression</p> <pre><code>- find: (!\\[.*?\\]\\()\\s*(?!cache)([^)]+)\\)\n\n- replace: $1cache/$2)\n</code></pre> <ul> <li><code>(!\\[.*?\\]\\()</code> matches the image tag</li> <li><code>\\s*</code> matches any whitespace after the image tag</li> <li><code>(?!cache)</code> is a negative lookahead that ensures the path does not already contain <code>cache</code></li> <li><code>([^)]+)</code> captures the rest of the path until the closing parenthesis<ul> <li><code>[^\\)]</code> matches any character that is not a closing parenthesis <code>)</code></li> </ul> </li> <li><code>$1cache/$2)</code> replaces it with the captured image tag and adds <code>cache/</code> before the rest of the path</li> </ul>"},{"location":"notes/angular/","title":"Angular","text":""},{"location":"notes/angular/#angular-overview","title":"Angular Overview","text":"<ul> <li>maintained by Google </li> <li>Platform:<ul> <li>CLI: <code>ng</code></li> <li>Angular Material: UI Components Library</li> <li>Angular Universal: SSR</li> </ul> </li> <li>Framework: offers libraries for routing and others<ul> <li>Modules (NgModules): Group components/services together</li> <li>Components: UI building blocks with HTML/CSS/TS logic</li> <li>Templates: Define the view layout using Angular syntax</li> <li>Services: Business logic, typically injected via Dependency Injection (DI)</li> <li>Routing: Navigates between components/pages</li> <li>RxJS: Handles asynchronous events/reactive data flows</li> <li>Directives/Pipes: Modify DOM or transform data in views</li> <li>NgRx: Redux-style state management for large-scale apps</li> </ul> </li> <li>Language: Primarily Typescript</li> <li>Rendering: Support server-side rendering (SSR) (using Angular Universal), or HR</li> </ul>"},{"location":"notes/angular/#angular-features","title":"Angular features","text":"<ol> <li>Component Base Architecture<ul> <li>encapsulated</li> <li>versatile with dependency architecture <ul> <li>Modular, loosely-coupled, testable </li> </ul> </li> </ul> </li> <li>Reactivity<ul> <li>uses Angular Signals</li> <li>compiler time optimizations?</li> </ul> </li> <li>SSR, SSG<ul> <li>Server send completed files to client</li> </ul> </li> <li>Router<ul> <li>navigation toolkit, route guards, data resolution, lazy-loading</li> </ul> </li> <li>form</li> <li>Open source</li> <li>Internationalization (i18n)<ul> <li><code>translationService</code></li> </ul> </li> </ol>"},{"location":"notes/angular/#comparison-to-react","title":"comparison to React","text":"Criteria Angular React Vue Type Full-fledged framework UI library Progressive framework Language TypeScript (officially) JavaScript / TypeScript JavaScript / TypeScript Architecture Opinionated (MVC-like) Flexible (V in MVC) Flexible, similar to React + Angular Data Binding Two-way + one-way One-way only Two-way (via <code>v-model</code>) Dependency Injection Built-in External libraries needed Basic reactivity, optional DI plugins CLI Tooling Very powerful (Angular CLI) Create React App / Vite Vue CLI / Vite Routing Official and integrated External (e.g., React Router) Official (Vue Router) State Management Optional NgRx (Redux-like) Redux, MobX, Zustand, etc. Vuex / Pinia Learning Curve Steep (more concepts) Moderate Gentle Enterprise Support Very strong (used by Google) Strong (used by Meta and community) Growing rapidly, Alibaba &amp; others use it"},{"location":"notes/angular/#get-started","title":"Get Started","text":"<ul> <li>Node.js - v^18.19.1 or newer</li> <li>Angular CLI: <code>npm install -g @angular/cli</code></li> </ul>"},{"location":"notes/angular/cli/","title":"Angular CLI tool","text":""},{"location":"notes/angular/cli/#install-angular-cli","title":"Install Angular CLI","text":"<pre><code>npm install -g @angular/cli\n</code></pre>"},{"location":"notes/angular/cli/#common-used-cli-commands","title":"common used CLI commands","text":"<pre><code>ng new my-app # create a new Angular app\nng serve # run the app in development mode\nng build # build the app for production\nng test # run unit tests\nng test --include src/app/path-to-your-file/your-file.spec.ts # run a specific test file\nng create component path/to/my-component # create a new component\n</code></pre>"},{"location":"notes/angular/component/","title":"Component","text":""},{"location":"notes/angular/component/#component_1","title":"<code>@Component</code>","text":"<ol> <li><code>style</code>/<code>styleUrl</code>: a CSS stylesheet defines the style of component</li> <li><code>template</code>/<code>templateUrl</code>: defines theHTML that renders into DOM</li> <li><code>selector</code> : defines the HTML tag of the component, equivalent to <code>customElements.define(CustomComponent, custom)</code></li> </ol>"},{"location":"notes/angular/component/#use-other-components-to-build-page","title":"use other components to build page","text":"<p>add <code>imports: [comp1, comp2, ....]</code> to import the components we need</p> <pre><code>import {UserLoginComponent} from \"user-login.ts\"\n@Component({\n    selector: 'user-profile',\n    templateUrl: 'user-profile.html'\n    styleUrl: 'user-profile.css',\n    imports: [UserLoginComponent]\n})\nexport class UserProfileComponent() {...}\n</code></pre>"},{"location":"notes/angular/httpClient/","title":"HTTP Client","text":"<p>from angular <code>@angular/common/http</code> package</p> <ul> <li>provides simplified HTTP request API</li> </ul>"},{"location":"notes/angular/httpClient/#convert-to-promise-chaining","title":"Convert to Promise Chaining:","text":"<p>simply do:</p> <pre><code>this.http.get&lt;&gt;(\"/api/users\").toPromise();\n</code></pre> <p>or if we cannot change the existing function:</p> <pre><code>function httpRequestWithCB(param, cb, cbErr);\n\nfunction convertToPromise(param) {\n    new Promise&lt;dataType&gt;((resolve, reject) =&gt; {\n        httpRequestWithCB(\n            param,\n            (data) =&gt; resolve(data),\n            (err) =&gt; reject(err)\n        );\n    }).then(...)\n}\n</code></pre>"},{"location":"notes/angular/signal/","title":"Signal","text":"<p>Create and Manage dynamic data, reactivity, manage state</p>"},{"location":"notes/angular/signal/#use-of-signal","title":"Use of Signal","text":"<pre><code>import { signal } from \"@angular/core\";\n// Create a signal with the `signal` function.\nconst firstName = signal(\"Morgan\");\n// **Read** a signal value by calling it\u2014 signals are functions.\nconsole.log(firstName());\n// **Change** the value of this signal by calling its `**set**` method with a new value.\nfirstName.set(\"Jaime\");\n// You can also use the `**update**` method to change the value\n// based on the **previous value**.\nfirstName.update((name) =&gt; name.toUpperCase());\n</code></pre>"},{"location":"notes/angular/signal/#types-of-signal","title":"Types of Signal","text":""},{"location":"notes/angular/signal/#computed-signal","title":"<code>computed</code> Signal","text":"<p>produce its value based on other signals read-only</p> <p>auto updates changes when any of the signal it reads changes, think a getter function</p> <pre><code>const name = signal(\"Mor\");\nconst nameCap = compute(() =&gt; name().toUpperCase());\n\nconsole.log(nameCap()); //MOR\n\nname.set(\"za\");\nconsole.log(nameCap()); //ZA\n</code></pre>"},{"location":"notes/angular/signal/#example","title":"Example","text":""},{"location":"notes/angular/signal/#simulate-a-counter","title":"Simulate a counter:","text":"DescriptionSolution <pre><code>import { Component, signal } from '@angular/core';\n\n@Component({\nselector: 'app-counter',\ntemplate: `\n    &lt;div&gt;\n    &lt;h2&gt;Count: {{ count() }}&lt;/h2&gt;\n    &lt;button (click)=\"increment()\"&gt;Increment&lt;/button&gt;\n    &lt;button (click)=\"decrement()\"&gt;Decrement&lt;/button&gt;\n    &lt;button (click)=\"reset()\"&gt;Reset&lt;/button&gt;\n    &lt;/div&gt;\n`\n    })\n    export class CounterComponent {\n    // Create a signal with initial value 0\n    __________________\n\n    increment() {\n        // Update signal value\n        __________________\n    }\n\n    decrement() {\n        __________________\n    }\n\n    reset() {\n        // Set signal to specific value\n        __________________\n    }\n}\n</code></pre> <pre><code>import { Component, signal } from '@angular/core';\n\n@Component({\nselector: 'app-counter',\ntemplate: `\n    &lt;div&gt;\n    &lt;h2&gt;Count: {{ count() }}&lt;/h2&gt;\n    &lt;button (click)=\"increment()\"&gt;Increment&lt;/button&gt;\n    &lt;button (click)=\"decrement()\"&gt;Decrement&lt;/button&gt;\n    &lt;button (click)=\"reset()\"&gt;Reset&lt;/button&gt;\n    &lt;/div&gt;\n`\n    })\n    export class CounterComponent {\n    // Create a signal with initial value 0\n    count = signal(0);\n\n    increment() {\n        // Update signal value\n        this.count.update(value =&gt; value + 1);\n    }\n\n    decrement() {\n        this.count.update(value =&gt; value - 1);\n    }\n\n    reset() {\n        // Set signal to specific value\n        this.count.set(0);\n    }\n}\n</code></pre>"},{"location":"notes/angular/signal/#array-stat","title":"Array Stat","text":"DescriptionSolutionOutput <p>given a dynamic array of numbers in <code>signal</code>, create dynamic accessors that returns Total value, average, and length</p> <pre><code>items = signal&lt;Array&lt;number&gt;&gt;([1,2,3,4,5,6]);\n</code></pre> array statistics<pre><code>import { Component, signal, computed } from \"@angular/core\";\nimport { bootstrapApplication } from \"@angular/platform-browser\";\n\n@Component({\n    selector: \"app-root\",\n    template: `\n        &lt;p&gt;{{ items }}&lt;/p&gt;\n        &lt;p&gt;total: {{ total }}&lt;/p&gt;\n        &lt;p&gt;avg: {{ avg }}&lt;/p&gt;\n        &lt;p&gt;count: {{ count }}&lt;/p&gt;\n    `,\n})\nexport class PlaygroundComponent {\n    items = signal&lt;Array&lt;number&gt;&gt;([1, 2, 3, 4, 5, 6]);\n    total = computed(() =&gt; this.items().reduce((sum, n) =&gt; sum + n, 0)); // (1)\n    count = computed(() =&gt; this.items().length);\n    avg = computed(() =&gt; this.total() / this.count());\n}\n</code></pre> <ol> <li>Array.reduce()</li> </ol> <pre><code>[Signal: 1,2,3,4,5,6]\n\ntotal: [Computed: 21]\n\navg: [Computed: 3.5]\n\ncount: [Computed: 6]\n</code></pre>"},{"location":"notes/angular/angular_material/form_control/","title":"Angular Material Form Control","text":"<p>we can create custom form field controls inside <code>&lt;mat-firn-field&gt;</code></p> <p>each form should have the following properties:</p> <pre><code>&lt;div [formGroup] = \"form-name\"&gt;\n    &lt;mat-form-field&gt;\n        &lt;mat-label&gt;custom form label&lt;/mat-label&gt;\n        &lt;mat-component formControlName=\"mc\"&gt;&lt;/mat-component&gt;\n        &lt;mat-icon matSuffix&gt;icon-name&lt;/mat-icon&gt;\n        &lt;mat-hint&gt;hint&lt;/mat-hint&gt;\n    &lt;/mat-form-field&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"notes/angular/angular_material/material_component/","title":"Angular Material","text":""},{"location":"notes/angular/angular_material/material_component/#topics","title":"Topics","text":"<ol> <li>Custom Form Control</li> </ol>"},{"location":"notes/angular/angular_material/material_component/#installation","title":"installation","text":"<p><code>ng add @angular/material</code> `ng add @angular/mat</p>"},{"location":"notes/angular/angular_material/material_component/#run","title":"run","text":"<p><code>ng serve</code></p>"},{"location":"notes/angular/angular_material/material_component/#displayuse-a-prebuilt-component","title":"display/use a prebuilt component","text":"<p>for example, if we want to use the mat slider, add following:</p> <p>=== HTML</p> <pre><code>```html\n&lt;mat-slider&gt;\n&lt;input matSliderThumb&gt;\n&lt;/mat-slider&gt;\n```\n</code></pre> <p>=== TS</p> <pre><code>```ts\nimport {Component} from '@angular/core';\nimport {MatSliderModule} from '@angular/material/slider';\n\n/**\n * @title Basic slider\n */\n@Component({\nselector: 'slider-overview-example',\ntemplateUrl: 'slider-overview-example.html',\nstyleUrl: 'slider-overview-example.css',\nimports: [MatSliderModule],\n})\nexport class SliderOverviewExample {}\n```\n</code></pre>"},{"location":"notes/javascript/","title":"Javascript","text":""},{"location":"notes/javascript/ds/arrays/","title":"Arrays","text":""},{"location":"notes/javascript/ds/arrays/#arrayprototypereduce","title":"<code>Array.prototype.reduce()</code>","text":""},{"location":"notes/javascript/ds/arrays/#signature","title":"Signature","text":"<p><code>reduce(callbackFn, initialValue?)</code></p>"},{"location":"notes/javascript/ds/arrays/#exception","title":"Exception","text":"<p><code>TypeError</code>: throw error if array is empty, AND <code>initialValue</code> is not provided</p>"},{"location":"notes/javascript/ds/arrays/#workflow","title":"workflow","text":"<ul> <li>an iterative method</li> <li>runs <code>cbfn</code> over all elements in the array in ascending order</li> <li>run <code>cbfn(ans, element) =&gt; ans</code> until element is null</li> <li>NOT invoked for empty element</li> <li>NOT support <code>thisArg</code> = <code>undefined</code>, gets substituted with <code>globalThis</code> if non-strict</li> </ul>"},{"location":"notes/javascript/ds/arrays/#edge-case","title":"Edge case","text":"<ul> <li>default <code>initialValue = 0</code></li> </ul>"},{"location":"notes/javascript/ds/arrays/#example-get-max","title":"example, get Max","text":"<pre><code>[1,5,2,12,124,54].reduce((ans, n) =&gt; Math.max(ans, n), 30); //124\n</code></pre>"},{"location":"notes/javascript/ds/arrays/#example-get-sum","title":"example, get sum","text":"<pre><code>[1,5,2,12,124,54].reduce((ans, n) =&gt; ans + n, 30); //124\n</code></pre>"},{"location":"notes/mkdocs/","title":"MKDocs","text":""},{"location":"notes/mkdocs/common_errors/","title":"Common Errors in MkDocs and Solutions","text":""},{"location":"notes/mkdocs/common_errors/#error-cannot-find-module-materialextensionsemoji-no-module-named-material","title":"Error: cannot find module 'material.extensions.emoji' (No module named 'material')","text":"<p>details:  <pre><code>PS P:\\Coding\\Kiminus\\docs&gt; mkdocs serve\nError: MkDocs encountered an error parsing the configuration file: while constructing a Python object\ncannot find module 'material.extensions.emoji' (No module named 'material')\n  in \"P:\\Coding\\Kiminus\\docs\\mkdocs.yml\", line 37, column 20\nPS P:\\Coding\\Kiminus\\docs&gt; \n</code></pre> solution: - make sure to activate the python virtual environment, then serve <pre><code>PS P:\\Coding\\Kiminus\\docs&gt; ../venv/Scripts/activate\n(venv) PS P:\\Coding\\Kiminus\\docs&gt; mkdocs serve\n</code></pre></p>"},{"location":"notes/typescript/","title":"Typescript","text":"<p>Typescript is an superset of Javascript</p>"},{"location":"notes/typescript/#run-ts-script","title":"run ts script","text":"<pre><code>npx ts-node script.ts\n</code></pre> <p>Typescript</p> <p>we need to first convert it to <code>unknown</code>, then convert to desired type by <code>data as Datatype</code></p>"},{"location":"notes/typescript/decorator/","title":"Typescript Decorator","text":"<p> download source ts</p>"},{"location":"notes/typescript/decorator/#decorator-workflow","title":"Decorator Workflow","text":"<pre><code>flowchart TD\n    A[Decorator Definition] --&gt; B{Decorator Type}\n\n    B --&gt;|Class Decorator| C1[Modify Class Constructor]\n    B --&gt;|Method Decorator| C2[Modify Method Behavior]\n    B --&gt;|Property Decorator| C3[Modify Property Access]\n    B --&gt;|Parameter Decorator| C4[Modify Parameter Metadata]\n\n    C1 --&gt; D1[Wrap/Extend Class Functionality]\n    C2 --&gt; D2[Modify Method Execution]\n    C3 --&gt; D3[Add Validation/Transformation]\n    C4 --&gt; D4[Add Metadata/Validation]\n\n    D1 --&gt; E1[Runtime Class Modification]\n    D2 --&gt; E2[Intercept Method Calls]\n    D3 --&gt; E3[Control Property Get/Set]\n    D4 --&gt; E4[Enhance Method Signatures]\n\n    E1 --&gt; F1[Additional Capabilities]\n    E2 --&gt; F2[Logging/Performance Tracking]\n    E3 --&gt; F3[Validation/Transformation]\n    E4 --&gt; F4[Dependency Injection]\n\n    F1 --&gt; G[Apply to Target]\n    F2 --&gt; G\n    F3 --&gt; G\n    F4 --&gt; G\n\n    G --&gt; H[Runtime Execution]\n    H --&gt; I{Conditions Met?}\n    I --&gt;|Yes| J[Execute Original Behavior]\n    I --&gt;|No| K[Handle/Modify Behavior]\n\n    J --&gt; L[Return Result]\n    K --&gt; L</code></pre>"},{"location":"notes/typescript/decorator/#decorator-composition","title":"Decorator Composition","text":"<p>Multiple decorators can be applied to a declaration, for example on a single line:</p> <pre><code>@f @g x\nmethod() { ... }\n</code></pre>"},{"location":"notes/typescript/decorator/#declare-a-decorator-factory","title":"Declare a Decorator Factory","text":"<pre><code>  function ActualDecorator(\n    target: any, // (1)\n    propertyKey: string, // (2)\n    descriptor: PropertyDescriptor) { // (3)\n    ...\n  }\n</code></pre> <ol> <li><code>target</code> The target is the constructor function if we apply the decorator to a static member and the    prototype of the class if it is applied on an instance property.</li> <li><code>propertyKey</code> is the name of the decorated method, example: <code>add</code></li> <li><code>descriptor</code> is the property descriptor of that method.     <pre><code>interface PropertyDescriptor {\n    configurable?: boolean;\n    enumerable?: boolean;\n    value?: any;\n    writable?: boolean;\n}\n</code></pre></li> </ol>"},{"location":"notes/typescript/decorator/#decorator-evaluation-order","title":"Decorator Evaluation order:","text":"<ol> <li>Parameter Decorators, followed by Method, Accessor, or Property Decorators are applied for each instance    member.</li> <li>Parameter Decorators, followed by Method, Accessor, or Property Decorators are applied for each static member.</li> <li>Parameter Decorators are applied for the constructor.</li> <li>Class Decorators are applied for the class.</li> </ol>"},{"location":"notes/typescript/decorator/#class-decorator","title":"class decorator","text":"DecoratorUsageOutput Class Decorator<pre><code>function LogClassDecorator(constructor: Function) {\n    console.log(\"constructor:\", constructor);\n}\n</code></pre> <pre><code>@LogClassDecorator\nclass Foo { ...}\n</code></pre> <pre><code>constructor: [class Foo] # (1)\n</code></pre> <ol> <li>accessible by <code>constructor.name</code></li> </ol>"},{"location":"notes/typescript/decorator/#method-decorators","title":"Method Decorators","text":"DecoratorUsageOutput <pre><code>function LogDebugInfo(\n  pattern: string = \"[{name}]({params}) -&gt; {result}\\nTime {time} ms\") {\n  return function (\n    target: any,\n    propertyKey: string,\n    descriptor:PropertyDescriptor){\n    const originalMethod = descriptor.value; // (1)!\n    descriptor.value = function (...args: any[]) { // (2)\n        const start = performance.now(); // (3) (4)?\n        const res = originalMethod.apply(this, args);\n        const end = performance.now();\n        const time = end - start;\n        console.log(\n            pattern\n                .replace(\"{name}\", propertyKey)\n                .replace(\"{params}\", args.join(\", \"))\n                .replace(\"{result}\", res)\n                .replace(\"{time}\", time.toString())\n        );\n    };\n  };\n}\n</code></pre> <ol> <li><code>descriptor.value</code> is a <code>Function</code> interface, <code>descriptor.value.toString()</code> =&gt;   <pre><code>add(a, b) {\n    return a + b;\n}\n</code></pre></li> <li><code>descriptor.value</code> is the decorated function that will be called. when the function is called, <code>args</code> will be passed in.</li> <li><code>performance</code> is from Performance API. which provides high-resolution timestamps and methods to measure the performance of your code</li> <li><code>performance.now()</code> gives the time relative to the time the page or Node.js process started.</li> </ol> <pre><code>@LogDebugInfo()\nfunction add(a: number, b: number): number {\n    return a + b;\n}\n\nconsole.log(add(1, 2)); // Output: 3\n</code></pre> <pre><code>[add](1, 2) -&gt; 3\nExec Time: 0.010499999999979082 ms\n</code></pre>"},{"location":"notes/typescript/decorator/#accessor-decorator","title":"Accessor Decorator","text":"DecoratorUsageOutput <pre><code>function LogAccessorDecorator(target: any, key: string, descriptor: PropertyDescriptor) {\n    console.log(\"target:\", target);\n    console.log(\"key:\", key);\n    console.log(\"descriptor:\", descriptor);\n  }\n</code></pre> <pre><code>class Foo {\n    private _pv: number;\n    @LogAccessorDecorator\n    get pv(): number {\n        return this._pv;\n    }\n}\n</code></pre> <pre><code>target: {}\nkey: pv\ndescriptor: {\n  get: [Function: get pv],\n  set: undefined,\n  enumerable: false,\n  configurable: true\n}\n</code></pre>"},{"location":"notes/typescript/decorator/#property-decorator","title":"Property Decorator","text":"DecoratorUsageOutput <pre><code>function LogPropertyDecorator(target: any, key: string) {\n  console.log(\"target:\", target);\n  console.log(\"key:\", key);\n}\n</code></pre> <pre><code>class Foo {\n  @LogPropertyDecorator\n  private _pv: number;\n}\n</code></pre> <pre><code>target: {}\nkey: _pv\n</code></pre>"},{"location":"notes/typescript/decorator/#parameter-decorator","title":"Parameter Decorator","text":"DecoratorUsageOutput <pre><code>function LogParameterDecorator(target: any, key: string) {\n    console.log(\"target:\", target);\n    console.log(\"key:\", key);\n}\n</code></pre> <pre><code>@LogParameterDecorator\nid: number = 0;\n</code></pre> <pre><code>target: {}\nkey: id\n</code></pre>"},{"location":"notes/typescript/decorator/#examples","title":"Examples:","text":""},{"location":"notes/typescript/decorator/#1-log-method-debug-info","title":"(1): Log Method Debug Info","text":"DescriptionDecoratorUsage <p>write a method decorator that logs the method name, arguments, and return value of the method, and its execution time.</p> <pre><code>  function LogDebugInfo(\n    target: any, // (1)\n    propertyKey: string, // (2)\n    descriptor: PropertyDescriptor) { // (3)\n       const originalMethod = descriptor.value;\n\n       descriptor.value = function (...args: any[]) {\n         const start = performance.now();\n         const result = originalMethod.apply(this, args);\n         const end = performance.now();\n         console.log(`Execution time for ${propertyKey}: ${end - start} ms`);\n         return result;\n       };\n\n       return descriptor;\n  }\n</code></pre> <pre><code> class Foo {\n     constructor() {\n         console.log('Foo constructor called');\n     }\n     @LogDebugInfo\n     add(a: number, b: number): number {\n         return a + b;\n     }\n     @LogDebugInfo\n     static multiply(a: number, b: number): number {\n         return a * b;\n     }\n }\n\n const foo = new Foo();\n console.log(foo.add(1, 2)); // Output: 3\n console.log(Foo.multiply(2, 3)); // Output: 6\n</code></pre>"},{"location":"notes/typescript/decorator/#2-min-field-length","title":"(2) Min Field Length","text":"DescriptionSolutionExample 1Usage 1Output 1 <p>Implement a property decorator @MinLength(len: number) that ensures a string property has at least len characters.</p> <pre><code>class User {\n  @MinLength(5)\n  username: string;\n\n  constructor(username: string) {\n    this.username = username;\n  }\n}\n</code></pre> <pre><code>function MinLength(len: number) {\n  return function (target: any, propertyKey: string) {\n    let value: string;\n    const descriptor: PropertyDescriptor = {\n        get() {\n            return value;\n        },\n        set(newValue: string) {\n            if (newValue.length &lt; len) {\n                throw new Error(`${propertyKey} must be at least ${len} characters long`);\n            }\n            value = newValue;\n        },\n        enumerable: true,\n        configurable: true,\n    };\n    Object.defineProperty(target, propertyKey, descriptor);\n  };\n}\n</code></pre> <pre><code>// Detailed implementation with console logs to track each stage\nfunction MinLength(len: number) {\n    console.log(`[Decorator Factory] MinLength called with length: ${len}`);\n\n    // Decorator function that will be applied to the property\n    return function(target: any, propertyKey: string) {\n        console.log(`[Decorator] Applied to target:`, target);\n        console.log(`[Decorator] Property key: ${propertyKey}`);\n\n        // Storage for the actual value\n        let value: string;\n\n        // Create property descriptor\n        const descriptor: PropertyDescriptor = {\n            // Getter\n            get() {\n                console.log(`[Getter] Retrieving value: ${value}`);\n                return value;\n            },\n\n            // Setter with validation\n            set(newValue: string) {\n                console.log(`[Setter] Attempting to set value: ${newValue}`);\n\n                // Validation logic\n                if (newValue.length &lt; len) {\n                    console.log(`[Validation] Length check failed`);\n                    throw new Error(`${propertyKey} must be at least ${len} characters long`);\n                }\n\n                console.log(`[Validation] Length check passed`);\n                value = newValue;\n            },\n            enumerable: true,\n            configurable: true\n        };\n\n        // Replace the property with our custom descriptor\n        console.log(`[Property Definition] Defining property with custom descriptor`);\n        Object.defineProperty(target, propertyKey, descriptor);\n    };\n}\n\n// User class with the decorator\nclass User {\n    @MinLength(5)\n    username: string;\n\n    constructor(username: string) {\n        console.log(`[Constructor] Creating user with username: ${username}`);\n        this.username = username;\n    }\n}\n</code></pre> <pre><code>// Demonstration of different scenarios\nconsole.log(\"--- Scenario 1: Valid Username ---\");\ntry {\n    const user = new User(\"johndoe\");\n    console.log(`Created user with username: ${user.username}`);\n} catch (error: any) {\n    console.error(error.message);\n}\n\nconsole.log(\"\\n--- Scenario 2: Invalid Username ---\");\ntry {\n    const user = new User(\"john\");\n} catch (error: any) {\n    console.error(error.message);\n}\n\nconsole.log(\"\\n--- Scenario 3: Changing Username After Creation ---\");\ntry {\n    const user = new User(\"johndoe\");\n    console.log(`Initial username: ${user.username}`);\n\n    // Attempt to change username\n    user.username = \"a\";\n} catch (error: any) {\n    console.error(error.message);\n}\n</code></pre> <pre><code>[Decorator Factory] MinLength called with length: 5\n[Decorator] Applied to target: {}\n[Decorator] Property key: username\n[Property Definition] Defining property with custom descriptor\n--- Scenario 1: Valid Username ---\n[Constructor] Creating user with username: johndoe\n[Setter] Attempting to set value: johndoe\n[Validation] Length check passed\n[Getter] Retrieving value: johndoe\nCreated user with username: johndoe\n\n--- Scenario 2: Invalid Username ---\n[Constructor] Creating user with username: john\n[Setter] Attempting to set value: john\n[Validation] Length check failed\nusername must be at least 5 characters long\n\n--- Scenario 3: Changing Username After Creation ---\n[Constructor] Creating user with username: johndoe\n[Setter] Attempting to set value: johndoe\n[Validation] Length check passed\n[Getter] Retrieving value: johndoe\nInitial username: johndoe\n[Setter] Attempting to set value: a\n[Validation] Length check failed\nusername must be at least 5 characters long\n</code></pre>"}]}